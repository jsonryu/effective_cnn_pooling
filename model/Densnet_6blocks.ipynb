{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torch.utils import data as D\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "validation_ratio = 0.1\n",
    "random_seed = 10\n",
    "initial_lr = 0.1\n",
    "num_epoch = 300\n",
    "# ptype -> 'max', 'avg', 'gauss_HWCN', 'gauss_CN', 'gauss_half_HWCN', 'gauss_half_CN'\n",
    "ptype = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "validset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_validation)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(validation_ratio * num_train))\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bn_relu_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n",
    "        super(bn_relu_conv, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(nin)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bottleneck_layer(nn.Sequential):\n",
    "  def __init__(self, nin, growth_rate, drop_rate=0.2):    \n",
    "      super(bottleneck_layer, self).__init__()\n",
    "      \n",
    "      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n",
    "      self.add_module('conv_3x3', bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "      \n",
    "      self.drop_rate = drop_rate\n",
    "      \n",
    "  def forward(self, x):\n",
    "      bottleneck_output = super(bottleneck_layer, self).forward(x)\n",
    "      if self.drop_rate > 0:\n",
    "          bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n",
    "          \n",
    "      bottleneck_output = torch.cat((x, bottleneck_output), 1)\n",
    "      \n",
    "      return bottleneck_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPooling2d(nn.AvgPool2d):\n",
    "    def __init__(self, num_features, kernel_size, stride=None, padding=0, ceil_mode=False,\n",
    "                 count_include_pad=True, hidden_node=None, stochasticity='HWCN', eps=1e-6):\n",
    "        if stochasticity != 'HWCN' and stochasticity != 'CN' and stochasticity is not None:\n",
    "            raise ValueError(\"gaussian pooling stochasticity has to be 'HWCN'/'CN' or None, \"\n",
    "                         \"but got {}\".format(stochasticity))\n",
    "        if hidden_node is None:\n",
    "            hidden_node = num_features // 2\n",
    "\n",
    "        super(GaussianPooling2d, self).__init__(kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode,\n",
    "                    count_include_pad=count_include_pad)\n",
    "        self.eps = eps\n",
    "        self.stochasticity = stochasticity\n",
    "\n",
    "        self.ToHidden = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(num_features, hidden_node, kernel_size=1,  padding=0, bias=True),\n",
    "            nn.BatchNorm2d(hidden_node),\n",
    "            nn.ReLU(False),\n",
    "        )\n",
    "        self.ToMean = nn.Sequential(\n",
    "            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n",
    "            nn.BatchNorm2d(num_features),\n",
    "        )\n",
    "        self.ToSigma = nn.Sequential(\n",
    "            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n",
    "            nn.BatchNorm2d(num_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.activation = nn.Softplus()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        mu0 = F.avg_pool2d(input, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n",
    "        sig0= F.avg_pool2d(input**2, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n",
    "        sig0= torch.sqrt(torch.clamp(sig0 - mu0**2, self.eps))\n",
    "\n",
    "        Z = self.ToHidden(input)\n",
    "        MU = self.ToMean(Z)\n",
    "\n",
    "        if self.training and self.stochasticity is not None:\n",
    "            SIGMA = self.ToSigma(Z)\n",
    "            if self.stochasticity == 'HWCN':\n",
    "                size = sig0.size()\n",
    "            else:\n",
    "                size = [sig0.size(0), sig0.size(1), 1, 1]\n",
    "            W = self.activation(MU + SIGMA * \n",
    "                torch.randn(size, dtype=sig0.dtype, layout=sig0.layout, device=sig0.device))\n",
    "        else:\n",
    "            W = self.activation(MU)\n",
    "\n",
    "        return mu0 + W*sig0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class halfGaussianPooling2d(nn.AvgPool2d):\n",
    "    def __init__(self, num_features, kernel_size, stride=None, padding=0, ceil_mode=False,\n",
    "                 count_include_pad=True, hidden_node=None, stochasticity='HWCN', eps=1e-6):\n",
    "        if stochasticity != 'HWCN' and stochasticity != 'CN' and stochasticity is not None:\n",
    "            raise ValueError(\"gaussian pooling stochasticity has to be 'HWCN'/'CN' or None, \"\n",
    "                         \"but got {}\".format(stochasticity))\n",
    "        if hidden_node is None:\n",
    "            hidden_node = num_features // 2\n",
    "\n",
    "        super(halfGaussianPooling2d, self).__init__(kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode,\n",
    "                    count_include_pad=count_include_pad)\n",
    "        self.eps = eps\n",
    "        self.stochasticity = stochasticity\n",
    "\n",
    "        self.ToHidden = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(num_features, hidden_node, kernel_size=1,  padding=0, bias=True),\n",
    "            nn.BatchNorm2d(hidden_node),\n",
    "            nn.ReLU(False),\n",
    "        )\n",
    "        self.ToSigma = nn.Sequential(\n",
    "            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n",
    "            nn.BatchNorm2d(num_features),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        mu0 = F.avg_pool2d(input, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n",
    "        sig0= F.avg_pool2d(input**2, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n",
    "        sig0= torch.sqrt(torch.clamp(sig0 - mu0**2, self.eps))\n",
    "\n",
    "        Z = self.ToHidden(input)\n",
    "        SIGMA = self.ToSigma(Z)\n",
    "        \n",
    "        if self.training and self.stochasticity is not None:\n",
    "            if self.stochasticity == 'HWCN':\n",
    "                size = sig0.size()\n",
    "            else:\n",
    "                size = [sig0.size(0), sig0.size(1), 1, 1]\n",
    "            W = torch.abs(torch.randn(size, dtype=sig0.dtype, layout=sig0.layout, device=sig0.device)) * SIGMA\n",
    "        else:\n",
    "            W = (math.sqrt(2) / math.sqrt(math.pi)) * SIGMA\n",
    "\n",
    "        return mu0 + W*sig0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pooling(ptype, num_features, kernel_size, stride, padding=0):\n",
    "    if ptype == 'max':\n",
    "        pool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    elif ptype == 'avg':\n",
    "        pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    elif ptype == 'gauss_HWCN':\n",
    "        pool = GaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    elif ptype == 'gauss_CN':\n",
    "        pool = GaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding, stochasticity='CN')\n",
    "    elif ptype == 'gauss_half_HWCN':\n",
    "        pool = halfGaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    elif ptype == 'gauss_half_CN':\n",
    "        pool = halfGaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding, stochasticity='CN')\n",
    "    else:\n",
    "        raise ValueError(\"pooling type of {} is not supported\".format(ptype))\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition_layer(nn.Sequential):\n",
    "  def __init__(self, nin, theta=0.5):    \n",
    "      super(Transition_layer, self).__init__()\n",
    "      \n",
    "      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n",
    "      self.add_module('{}'.format(ptype), _pooling(ptype=ptype, num_features=int(nin*theta), kernel_size=2, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Sequential):\n",
    "  def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n",
    "      super(DenseBlock, self).__init__()\n",
    "                        \n",
    "      for i in range(num_bottleneck_layers):\n",
    "          nin_bottleneck_layer = nin + growth_rate * i\n",
    "          self.add_module('bottleneck_layer_%d' % i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        assert (num_layers - 7) % 12 == 0\n",
    "        \n",
    "        # (num_layers-4)//6 \n",
    "        num_bottleneck_layers = (num_layers - 7) // 12\n",
    "        \n",
    "        # 32 x 32 x 3 --> 32 x 32 x (growth_rate*2)\n",
    "        self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "                \n",
    "        # 32 x 32 x (growth_rate*2) --> 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]\n",
    "        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        # 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)] --> 17 x 17 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]*theta\n",
    "        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n",
    "        self.transition_layer_1 = Transition_layer(nin=nin_transition_layer_1, theta=theta)\n",
    "        \n",
    "        # 17 x 17 x nin_transition_layer_1*theta --> 17 x 17 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]\n",
    "        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        # 17 x 17 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)] --> 9 x 9 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]*theta\n",
    "        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n",
    "        self.transition_layer_2 = Transition_layer(nin=nin_transition_layer_2, theta=theta)\n",
    "        \n",
    "        # 9 x 9 x nin_transition_layer_2*theta --> 9 x 9 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]\n",
    "        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        \n",
    "        # 9 x 9 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> 5 x 5 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]*theta\n",
    "        nin_transition_layer_3 = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers)\n",
    "        self.transition_layer_3 = Transition_layer(nin=nin_transition_layer_3, theta=theta)\n",
    "        \n",
    "        # 5 x 5 x nin_transition_layer_3*theta --> 5 x 5 x [nin_transition_layer_3*theta + (growth_rate * num_bottleneck_layers)]\n",
    "        self.dense_block_4 = DenseBlock(nin=int(nin_transition_layer_3*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        \n",
    "        # 5 x 5 x [nin_transition_layer_3*theta + (growth_rate * num_bottleneck_layers)] --> 3 x 3 x [nin_transition_layer_3*theta + (growth_rate * num_bottleneck_layers)]*theta\n",
    "        nin_transition_layer_4 = int(nin_transition_layer_3*theta) + (growth_rate * num_bottleneck_layers)\n",
    "        self.transition_layer_4 = Transition_layer(nin=nin_transition_layer_4, theta=theta)\n",
    "        \n",
    "        # 3 x 3 x nin_transition_layer_4*theta --> 3 x 3 x [nin_transition_layer_4*theta + (growth_rate * num_bottleneck_layers)]\n",
    "        self.dense_block_5 = DenseBlock(nin=int(nin_transition_layer_4*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        \n",
    "        # 3 x 3 x [nin_transition_layer_4*theta + (growth_rate * num_bottleneck_layers)] --> 2 x 2 x [nin_transition_layer_4*theta + (growth_rate * num_bottleneck_layers)]*theta\n",
    "        nin_transition_layer_5 = int(nin_transition_layer_4*theta) + (growth_rate * num_bottleneck_layers)\n",
    "        self.transition_layer_5 = Transition_layer(nin=nin_transition_layer_5, theta=theta)\n",
    "        \n",
    "        # 2 x 2 x nin_transition_layer_5*theta --> 2 x 2 x [nin_transition_layer_5*theta + (growth_rate * num_bottleneck_layers)]\n",
    "        self.dense_block_6 = DenseBlock(nin=int(nin_transition_layer_5*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        \n",
    "        nin_fc_layer = int(nin_transition_layer_5*theta) + (growth_rate * num_bottleneck_layers) \n",
    "        \n",
    "        # [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> num_classes\n",
    "        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dense_init_output = self.dense_init(x)\n",
    "        \n",
    "        dense_block_1_output = self.dense_block_1(dense_init_output)\n",
    "        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n",
    "        \n",
    "        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n",
    "        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n",
    "        \n",
    "        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n",
    "        transition_layer_3_output = self.transition_layer_3(dense_block_3_output)\n",
    "        \n",
    "        dense_block_4_output = self.dense_block_4(transition_layer_3_output)\n",
    "        transition_layer_4_output = self.transition_layer_4(dense_block_4_output)\n",
    "        \n",
    "        dense_block_5_output = self.dense_block_5(transition_layer_4_output)\n",
    "        transition_layer_5_output = self.transition_layer_5(dense_block_5_output)\n",
    "        \n",
    "        dense_block_6_output = self.dense_block_6(transition_layer_5_output)\n",
    "        \n",
    "        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_6_output, (1, 1))                \n",
    "        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n",
    "\n",
    "        output = self.fc_layer(global_avg_pool_output_flat)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNetBC_100_12():\n",
    "    return DenseNet(growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10)\n",
    "\n",
    "def DenseNetBC_250_24():\n",
    "    return DenseNet(growth_rate=24, num_layers=250, theta=0.5, drop_rate=0.2, num_classes=10)\n",
    "\n",
    "def DenseNetBC_190_40():\n",
    "    return DenseNet(growth_rate=40, num_layers=190, theta=0.5, drop_rate=0.2, num_classes=10)\n",
    "\n",
    "def DenseNetBC_103_12():\n",
    "    return DenseNet(growth_rate=12, num_layers=103, theta=0.5, drop_rate=0.2, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DenseNetBC_103_12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (dense_init): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dense_block_1): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_1): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_2): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_3): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_4): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_5): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_6): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_7): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_layer_1): Transition_layer(\n",
       "    (conv_1x1): bn_relu_conv(\n",
       "      (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(120, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (max): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dense_block_2): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_1): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_2): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_3): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_4): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_5): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_6): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_7): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_layer_2): Transition_layer(\n",
       "    (conv_1x1): bn_relu_conv(\n",
       "      (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(156, 78, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (max): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dense_block_3): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(78, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_1): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(90, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_2): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(102, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_3): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(114, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_4): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(126, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(126, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_5): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(138, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(138, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_6): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_7): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_layer_3): Transition_layer(\n",
       "    (conv_1x1): bn_relu_conv(\n",
       "      (batch_norm): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(174, 87, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (max): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dense_block_4): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(87, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_1): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(99, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_2): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(111, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_3): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(123, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(123, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_4): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(135, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(135, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_5): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(147, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(147, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_6): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(159, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(159, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_7): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(171, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(171, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_layer_4): Transition_layer(\n",
       "    (conv_1x1): bn_relu_conv(\n",
       "      (batch_norm): BatchNorm2d(183, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(183, 91, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (max): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dense_block_5): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(91, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_1): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(103, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(103, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_2): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(115, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_3): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_4): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(139, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_5): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(151, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(151, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_6): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(163, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(163, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_7): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(175, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(175, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_layer_5): Transition_layer(\n",
       "    (conv_1x1): bn_relu_conv(\n",
       "      (batch_norm): BatchNorm2d(187, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(187, 93, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (max): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dense_block_6): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(93, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_1): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(105, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(105, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_2): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(117, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_3): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(129, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(129, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_4): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(141, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(141, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_5): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(153, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_6): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(165, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(165, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bottleneck_layer_7): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(177, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(177, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_layer): Linear(in_features=189, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 32, 32]             672\n",
      "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
      "              ReLU-3           [-1, 24, 32, 32]               0\n",
      "            Conv2d-4           [-1, 48, 32, 32]           1,152\n",
      "      bn_relu_conv-5           [-1, 48, 32, 32]               0\n",
      "       BatchNorm2d-6           [-1, 48, 32, 32]              96\n",
      "              ReLU-7           [-1, 48, 32, 32]               0\n",
      "            Conv2d-8           [-1, 12, 32, 32]           5,184\n",
      "      bn_relu_conv-9           [-1, 12, 32, 32]               0\n",
      "      BatchNorm2d-10           [-1, 36, 32, 32]              72\n",
      "             ReLU-11           [-1, 36, 32, 32]               0\n",
      "           Conv2d-12           [-1, 48, 32, 32]           1,728\n",
      "     bn_relu_conv-13           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-14           [-1, 48, 32, 32]              96\n",
      "             ReLU-15           [-1, 48, 32, 32]               0\n",
      "           Conv2d-16           [-1, 12, 32, 32]           5,184\n",
      "     bn_relu_conv-17           [-1, 12, 32, 32]               0\n",
      "      BatchNorm2d-18           [-1, 48, 32, 32]              96\n",
      "             ReLU-19           [-1, 48, 32, 32]               0\n",
      "           Conv2d-20           [-1, 48, 32, 32]           2,304\n",
      "     bn_relu_conv-21           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-22           [-1, 48, 32, 32]              96\n",
      "             ReLU-23           [-1, 48, 32, 32]               0\n",
      "           Conv2d-24           [-1, 12, 32, 32]           5,184\n",
      "     bn_relu_conv-25           [-1, 12, 32, 32]               0\n",
      "      BatchNorm2d-26           [-1, 60, 32, 32]             120\n",
      "             ReLU-27           [-1, 60, 32, 32]               0\n",
      "           Conv2d-28           [-1, 48, 32, 32]           2,880\n",
      "     bn_relu_conv-29           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-30           [-1, 48, 32, 32]              96\n",
      "             ReLU-31           [-1, 48, 32, 32]               0\n",
      "           Conv2d-32           [-1, 12, 32, 32]           5,184\n",
      "     bn_relu_conv-33           [-1, 12, 32, 32]               0\n",
      "      BatchNorm2d-34           [-1, 72, 32, 32]             144\n",
      "             ReLU-35           [-1, 72, 32, 32]               0\n",
      "           Conv2d-36           [-1, 48, 32, 32]           3,456\n",
      "     bn_relu_conv-37           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-38           [-1, 48, 32, 32]              96\n",
      "             ReLU-39           [-1, 48, 32, 32]               0\n",
      "           Conv2d-40           [-1, 12, 32, 32]           5,184\n",
      "     bn_relu_conv-41           [-1, 12, 32, 32]               0\n",
      "      BatchNorm2d-42           [-1, 84, 32, 32]             168\n",
      "             ReLU-43           [-1, 84, 32, 32]               0\n",
      "           Conv2d-44           [-1, 48, 32, 32]           4,032\n",
      "     bn_relu_conv-45           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-46           [-1, 48, 32, 32]              96\n",
      "             ReLU-47           [-1, 48, 32, 32]               0\n",
      "           Conv2d-48           [-1, 12, 32, 32]           5,184\n",
      "     bn_relu_conv-49           [-1, 12, 32, 32]               0\n",
      "      BatchNorm2d-50           [-1, 96, 32, 32]             192\n",
      "             ReLU-51           [-1, 96, 32, 32]               0\n",
      "           Conv2d-52           [-1, 48, 32, 32]           4,608\n",
      "     bn_relu_conv-53           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-54           [-1, 48, 32, 32]              96\n",
      "             ReLU-55           [-1, 48, 32, 32]               0\n",
      "           Conv2d-56           [-1, 12, 32, 32]           5,184\n",
      "     bn_relu_conv-57           [-1, 12, 32, 32]               0\n",
      "      BatchNorm2d-58          [-1, 108, 32, 32]             216\n",
      "             ReLU-59          [-1, 108, 32, 32]               0\n",
      "           Conv2d-60           [-1, 48, 32, 32]           5,184\n",
      "     bn_relu_conv-61           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-62           [-1, 48, 32, 32]              96\n",
      "             ReLU-63           [-1, 48, 32, 32]               0\n",
      "           Conv2d-64           [-1, 12, 32, 32]           5,184\n",
      "     bn_relu_conv-65           [-1, 12, 32, 32]               0\n",
      "      BatchNorm2d-66          [-1, 120, 32, 32]             240\n",
      "             ReLU-67          [-1, 120, 32, 32]               0\n",
      "           Conv2d-68           [-1, 60, 32, 32]           7,200\n",
      "     bn_relu_conv-69           [-1, 60, 32, 32]               0\n",
      "        MaxPool2d-70           [-1, 60, 17, 17]               0\n",
      "      BatchNorm2d-71           [-1, 60, 17, 17]             120\n",
      "             ReLU-72           [-1, 60, 17, 17]               0\n",
      "           Conv2d-73           [-1, 48, 17, 17]           2,880\n",
      "     bn_relu_conv-74           [-1, 48, 17, 17]               0\n",
      "      BatchNorm2d-75           [-1, 48, 17, 17]              96\n",
      "             ReLU-76           [-1, 48, 17, 17]               0\n",
      "           Conv2d-77           [-1, 12, 17, 17]           5,184\n",
      "     bn_relu_conv-78           [-1, 12, 17, 17]               0\n",
      "      BatchNorm2d-79           [-1, 72, 17, 17]             144\n",
      "             ReLU-80           [-1, 72, 17, 17]               0\n",
      "           Conv2d-81           [-1, 48, 17, 17]           3,456\n",
      "     bn_relu_conv-82           [-1, 48, 17, 17]               0\n",
      "      BatchNorm2d-83           [-1, 48, 17, 17]              96\n",
      "             ReLU-84           [-1, 48, 17, 17]               0\n",
      "           Conv2d-85           [-1, 12, 17, 17]           5,184\n",
      "     bn_relu_conv-86           [-1, 12, 17, 17]               0\n",
      "      BatchNorm2d-87           [-1, 84, 17, 17]             168\n",
      "             ReLU-88           [-1, 84, 17, 17]               0\n",
      "           Conv2d-89           [-1, 48, 17, 17]           4,032\n",
      "     bn_relu_conv-90           [-1, 48, 17, 17]               0\n",
      "      BatchNorm2d-91           [-1, 48, 17, 17]              96\n",
      "             ReLU-92           [-1, 48, 17, 17]               0\n",
      "           Conv2d-93           [-1, 12, 17, 17]           5,184\n",
      "     bn_relu_conv-94           [-1, 12, 17, 17]               0\n",
      "      BatchNorm2d-95           [-1, 96, 17, 17]             192\n",
      "             ReLU-96           [-1, 96, 17, 17]               0\n",
      "           Conv2d-97           [-1, 48, 17, 17]           4,608\n",
      "     bn_relu_conv-98           [-1, 48, 17, 17]               0\n",
      "      BatchNorm2d-99           [-1, 48, 17, 17]              96\n",
      "            ReLU-100           [-1, 48, 17, 17]               0\n",
      "          Conv2d-101           [-1, 12, 17, 17]           5,184\n",
      "    bn_relu_conv-102           [-1, 12, 17, 17]               0\n",
      "     BatchNorm2d-103          [-1, 108, 17, 17]             216\n",
      "            ReLU-104          [-1, 108, 17, 17]               0\n",
      "          Conv2d-105           [-1, 48, 17, 17]           5,184\n",
      "    bn_relu_conv-106           [-1, 48, 17, 17]               0\n",
      "     BatchNorm2d-107           [-1, 48, 17, 17]              96\n",
      "            ReLU-108           [-1, 48, 17, 17]               0\n",
      "          Conv2d-109           [-1, 12, 17, 17]           5,184\n",
      "    bn_relu_conv-110           [-1, 12, 17, 17]               0\n",
      "     BatchNorm2d-111          [-1, 120, 17, 17]             240\n",
      "            ReLU-112          [-1, 120, 17, 17]               0\n",
      "          Conv2d-113           [-1, 48, 17, 17]           5,760\n",
      "    bn_relu_conv-114           [-1, 48, 17, 17]               0\n",
      "     BatchNorm2d-115           [-1, 48, 17, 17]              96\n",
      "            ReLU-116           [-1, 48, 17, 17]               0\n",
      "          Conv2d-117           [-1, 12, 17, 17]           5,184\n",
      "    bn_relu_conv-118           [-1, 12, 17, 17]               0\n",
      "     BatchNorm2d-119          [-1, 132, 17, 17]             264\n",
      "            ReLU-120          [-1, 132, 17, 17]               0\n",
      "          Conv2d-121           [-1, 48, 17, 17]           6,336\n",
      "    bn_relu_conv-122           [-1, 48, 17, 17]               0\n",
      "     BatchNorm2d-123           [-1, 48, 17, 17]              96\n",
      "            ReLU-124           [-1, 48, 17, 17]               0\n",
      "          Conv2d-125           [-1, 12, 17, 17]           5,184\n",
      "    bn_relu_conv-126           [-1, 12, 17, 17]               0\n",
      "     BatchNorm2d-127          [-1, 144, 17, 17]             288\n",
      "            ReLU-128          [-1, 144, 17, 17]               0\n",
      "          Conv2d-129           [-1, 48, 17, 17]           6,912\n",
      "    bn_relu_conv-130           [-1, 48, 17, 17]               0\n",
      "     BatchNorm2d-131           [-1, 48, 17, 17]              96\n",
      "            ReLU-132           [-1, 48, 17, 17]               0\n",
      "          Conv2d-133           [-1, 12, 17, 17]           5,184\n",
      "    bn_relu_conv-134           [-1, 12, 17, 17]               0\n",
      "     BatchNorm2d-135          [-1, 156, 17, 17]             312\n",
      "            ReLU-136          [-1, 156, 17, 17]               0\n",
      "          Conv2d-137           [-1, 78, 17, 17]          12,168\n",
      "    bn_relu_conv-138           [-1, 78, 17, 17]               0\n",
      "       MaxPool2d-139             [-1, 78, 9, 9]               0\n",
      "     BatchNorm2d-140             [-1, 78, 9, 9]             156\n",
      "            ReLU-141             [-1, 78, 9, 9]               0\n",
      "          Conv2d-142             [-1, 48, 9, 9]           3,744\n",
      "    bn_relu_conv-143             [-1, 48, 9, 9]               0\n",
      "     BatchNorm2d-144             [-1, 48, 9, 9]              96\n",
      "            ReLU-145             [-1, 48, 9, 9]               0\n",
      "          Conv2d-146             [-1, 12, 9, 9]           5,184\n",
      "    bn_relu_conv-147             [-1, 12, 9, 9]               0\n",
      "     BatchNorm2d-148             [-1, 90, 9, 9]             180\n",
      "            ReLU-149             [-1, 90, 9, 9]               0\n",
      "          Conv2d-150             [-1, 48, 9, 9]           4,320\n",
      "    bn_relu_conv-151             [-1, 48, 9, 9]               0\n",
      "     BatchNorm2d-152             [-1, 48, 9, 9]              96\n",
      "            ReLU-153             [-1, 48, 9, 9]               0\n",
      "          Conv2d-154             [-1, 12, 9, 9]           5,184\n",
      "    bn_relu_conv-155             [-1, 12, 9, 9]               0\n",
      "     BatchNorm2d-156            [-1, 102, 9, 9]             204\n",
      "            ReLU-157            [-1, 102, 9, 9]               0\n",
      "          Conv2d-158             [-1, 48, 9, 9]           4,896\n",
      "    bn_relu_conv-159             [-1, 48, 9, 9]               0\n",
      "     BatchNorm2d-160             [-1, 48, 9, 9]              96\n",
      "            ReLU-161             [-1, 48, 9, 9]               0\n",
      "          Conv2d-162             [-1, 12, 9, 9]           5,184\n",
      "    bn_relu_conv-163             [-1, 12, 9, 9]               0\n",
      "     BatchNorm2d-164            [-1, 114, 9, 9]             228\n",
      "            ReLU-165            [-1, 114, 9, 9]               0\n",
      "          Conv2d-166             [-1, 48, 9, 9]           5,472\n",
      "    bn_relu_conv-167             [-1, 48, 9, 9]               0\n",
      "     BatchNorm2d-168             [-1, 48, 9, 9]              96\n",
      "            ReLU-169             [-1, 48, 9, 9]               0\n",
      "          Conv2d-170             [-1, 12, 9, 9]           5,184\n",
      "    bn_relu_conv-171             [-1, 12, 9, 9]               0\n",
      "     BatchNorm2d-172            [-1, 126, 9, 9]             252\n",
      "            ReLU-173            [-1, 126, 9, 9]               0\n",
      "          Conv2d-174             [-1, 48, 9, 9]           6,048\n",
      "    bn_relu_conv-175             [-1, 48, 9, 9]               0\n",
      "     BatchNorm2d-176             [-1, 48, 9, 9]              96\n",
      "            ReLU-177             [-1, 48, 9, 9]               0\n",
      "          Conv2d-178             [-1, 12, 9, 9]           5,184\n",
      "    bn_relu_conv-179             [-1, 12, 9, 9]               0\n",
      "     BatchNorm2d-180            [-1, 138, 9, 9]             276\n",
      "            ReLU-181            [-1, 138, 9, 9]               0\n",
      "          Conv2d-182             [-1, 48, 9, 9]           6,624\n",
      "    bn_relu_conv-183             [-1, 48, 9, 9]               0\n",
      "     BatchNorm2d-184             [-1, 48, 9, 9]              96\n",
      "            ReLU-185             [-1, 48, 9, 9]               0\n",
      "          Conv2d-186             [-1, 12, 9, 9]           5,184\n",
      "    bn_relu_conv-187             [-1, 12, 9, 9]               0\n",
      "     BatchNorm2d-188            [-1, 150, 9, 9]             300\n",
      "            ReLU-189            [-1, 150, 9, 9]               0\n",
      "          Conv2d-190             [-1, 48, 9, 9]           7,200\n",
      "    bn_relu_conv-191             [-1, 48, 9, 9]               0\n",
      "     BatchNorm2d-192             [-1, 48, 9, 9]              96\n",
      "            ReLU-193             [-1, 48, 9, 9]               0\n",
      "          Conv2d-194             [-1, 12, 9, 9]           5,184\n",
      "    bn_relu_conv-195             [-1, 12, 9, 9]               0\n",
      "     BatchNorm2d-196            [-1, 162, 9, 9]             324\n",
      "            ReLU-197            [-1, 162, 9, 9]               0\n",
      "          Conv2d-198             [-1, 48, 9, 9]           7,776\n",
      "    bn_relu_conv-199             [-1, 48, 9, 9]               0\n",
      "     BatchNorm2d-200             [-1, 48, 9, 9]              96\n",
      "            ReLU-201             [-1, 48, 9, 9]               0\n",
      "          Conv2d-202             [-1, 12, 9, 9]           5,184\n",
      "    bn_relu_conv-203             [-1, 12, 9, 9]               0\n",
      "     BatchNorm2d-204            [-1, 174, 9, 9]             348\n",
      "            ReLU-205            [-1, 174, 9, 9]               0\n",
      "          Conv2d-206             [-1, 87, 9, 9]          15,138\n",
      "    bn_relu_conv-207             [-1, 87, 9, 9]               0\n",
      "       MaxPool2d-208             [-1, 87, 5, 5]               0\n",
      "     BatchNorm2d-209             [-1, 87, 5, 5]             174\n",
      "            ReLU-210             [-1, 87, 5, 5]               0\n",
      "          Conv2d-211             [-1, 48, 5, 5]           4,176\n",
      "    bn_relu_conv-212             [-1, 48, 5, 5]               0\n",
      "     BatchNorm2d-213             [-1, 48, 5, 5]              96\n",
      "            ReLU-214             [-1, 48, 5, 5]               0\n",
      "          Conv2d-215             [-1, 12, 5, 5]           5,184\n",
      "    bn_relu_conv-216             [-1, 12, 5, 5]               0\n",
      "     BatchNorm2d-217             [-1, 99, 5, 5]             198\n",
      "            ReLU-218             [-1, 99, 5, 5]               0\n",
      "          Conv2d-219             [-1, 48, 5, 5]           4,752\n",
      "    bn_relu_conv-220             [-1, 48, 5, 5]               0\n",
      "     BatchNorm2d-221             [-1, 48, 5, 5]              96\n",
      "            ReLU-222             [-1, 48, 5, 5]               0\n",
      "          Conv2d-223             [-1, 12, 5, 5]           5,184\n",
      "    bn_relu_conv-224             [-1, 12, 5, 5]               0\n",
      "     BatchNorm2d-225            [-1, 111, 5, 5]             222\n",
      "            ReLU-226            [-1, 111, 5, 5]               0\n",
      "          Conv2d-227             [-1, 48, 5, 5]           5,328\n",
      "    bn_relu_conv-228             [-1, 48, 5, 5]               0\n",
      "     BatchNorm2d-229             [-1, 48, 5, 5]              96\n",
      "            ReLU-230             [-1, 48, 5, 5]               0\n",
      "          Conv2d-231             [-1, 12, 5, 5]           5,184\n",
      "    bn_relu_conv-232             [-1, 12, 5, 5]               0\n",
      "     BatchNorm2d-233            [-1, 123, 5, 5]             246\n",
      "            ReLU-234            [-1, 123, 5, 5]               0\n",
      "          Conv2d-235             [-1, 48, 5, 5]           5,904\n",
      "    bn_relu_conv-236             [-1, 48, 5, 5]               0\n",
      "     BatchNorm2d-237             [-1, 48, 5, 5]              96\n",
      "            ReLU-238             [-1, 48, 5, 5]               0\n",
      "          Conv2d-239             [-1, 12, 5, 5]           5,184\n",
      "    bn_relu_conv-240             [-1, 12, 5, 5]               0\n",
      "     BatchNorm2d-241            [-1, 135, 5, 5]             270\n",
      "            ReLU-242            [-1, 135, 5, 5]               0\n",
      "          Conv2d-243             [-1, 48, 5, 5]           6,480\n",
      "    bn_relu_conv-244             [-1, 48, 5, 5]               0\n",
      "     BatchNorm2d-245             [-1, 48, 5, 5]              96\n",
      "            ReLU-246             [-1, 48, 5, 5]               0\n",
      "          Conv2d-247             [-1, 12, 5, 5]           5,184\n",
      "    bn_relu_conv-248             [-1, 12, 5, 5]               0\n",
      "     BatchNorm2d-249            [-1, 147, 5, 5]             294\n",
      "            ReLU-250            [-1, 147, 5, 5]               0\n",
      "          Conv2d-251             [-1, 48, 5, 5]           7,056\n",
      "    bn_relu_conv-252             [-1, 48, 5, 5]               0\n",
      "     BatchNorm2d-253             [-1, 48, 5, 5]              96\n",
      "            ReLU-254             [-1, 48, 5, 5]               0\n",
      "          Conv2d-255             [-1, 12, 5, 5]           5,184\n",
      "    bn_relu_conv-256             [-1, 12, 5, 5]               0\n",
      "     BatchNorm2d-257            [-1, 159, 5, 5]             318\n",
      "            ReLU-258            [-1, 159, 5, 5]               0\n",
      "          Conv2d-259             [-1, 48, 5, 5]           7,632\n",
      "    bn_relu_conv-260             [-1, 48, 5, 5]               0\n",
      "     BatchNorm2d-261             [-1, 48, 5, 5]              96\n",
      "            ReLU-262             [-1, 48, 5, 5]               0\n",
      "          Conv2d-263             [-1, 12, 5, 5]           5,184\n",
      "    bn_relu_conv-264             [-1, 12, 5, 5]               0\n",
      "     BatchNorm2d-265            [-1, 171, 5, 5]             342\n",
      "            ReLU-266            [-1, 171, 5, 5]               0\n",
      "          Conv2d-267             [-1, 48, 5, 5]           8,208\n",
      "    bn_relu_conv-268             [-1, 48, 5, 5]               0\n",
      "     BatchNorm2d-269             [-1, 48, 5, 5]              96\n",
      "            ReLU-270             [-1, 48, 5, 5]               0\n",
      "          Conv2d-271             [-1, 12, 5, 5]           5,184\n",
      "    bn_relu_conv-272             [-1, 12, 5, 5]               0\n",
      "     BatchNorm2d-273            [-1, 183, 5, 5]             366\n",
      "            ReLU-274            [-1, 183, 5, 5]               0\n",
      "          Conv2d-275             [-1, 91, 5, 5]          16,653\n",
      "    bn_relu_conv-276             [-1, 91, 5, 5]               0\n",
      "       MaxPool2d-277             [-1, 91, 3, 3]               0\n",
      "     BatchNorm2d-278             [-1, 91, 3, 3]             182\n",
      "            ReLU-279             [-1, 91, 3, 3]               0\n",
      "          Conv2d-280             [-1, 48, 3, 3]           4,368\n",
      "    bn_relu_conv-281             [-1, 48, 3, 3]               0\n",
      "     BatchNorm2d-282             [-1, 48, 3, 3]              96\n",
      "            ReLU-283             [-1, 48, 3, 3]               0\n",
      "          Conv2d-284             [-1, 12, 3, 3]           5,184\n",
      "    bn_relu_conv-285             [-1, 12, 3, 3]               0\n",
      "     BatchNorm2d-286            [-1, 103, 3, 3]             206\n",
      "            ReLU-287            [-1, 103, 3, 3]               0\n",
      "          Conv2d-288             [-1, 48, 3, 3]           4,944\n",
      "    bn_relu_conv-289             [-1, 48, 3, 3]               0\n",
      "     BatchNorm2d-290             [-1, 48, 3, 3]              96\n",
      "            ReLU-291             [-1, 48, 3, 3]               0\n",
      "          Conv2d-292             [-1, 12, 3, 3]           5,184\n",
      "    bn_relu_conv-293             [-1, 12, 3, 3]               0\n",
      "     BatchNorm2d-294            [-1, 115, 3, 3]             230\n",
      "            ReLU-295            [-1, 115, 3, 3]               0\n",
      "          Conv2d-296             [-1, 48, 3, 3]           5,520\n",
      "    bn_relu_conv-297             [-1, 48, 3, 3]               0\n",
      "     BatchNorm2d-298             [-1, 48, 3, 3]              96\n",
      "            ReLU-299             [-1, 48, 3, 3]               0\n",
      "          Conv2d-300             [-1, 12, 3, 3]           5,184\n",
      "    bn_relu_conv-301             [-1, 12, 3, 3]               0\n",
      "     BatchNorm2d-302            [-1, 127, 3, 3]             254\n",
      "            ReLU-303            [-1, 127, 3, 3]               0\n",
      "          Conv2d-304             [-1, 48, 3, 3]           6,096\n",
      "    bn_relu_conv-305             [-1, 48, 3, 3]               0\n",
      "     BatchNorm2d-306             [-1, 48, 3, 3]              96\n",
      "            ReLU-307             [-1, 48, 3, 3]               0\n",
      "          Conv2d-308             [-1, 12, 3, 3]           5,184\n",
      "    bn_relu_conv-309             [-1, 12, 3, 3]               0\n",
      "     BatchNorm2d-310            [-1, 139, 3, 3]             278\n",
      "            ReLU-311            [-1, 139, 3, 3]               0\n",
      "          Conv2d-312             [-1, 48, 3, 3]           6,672\n",
      "    bn_relu_conv-313             [-1, 48, 3, 3]               0\n",
      "     BatchNorm2d-314             [-1, 48, 3, 3]              96\n",
      "            ReLU-315             [-1, 48, 3, 3]               0\n",
      "          Conv2d-316             [-1, 12, 3, 3]           5,184\n",
      "    bn_relu_conv-317             [-1, 12, 3, 3]               0\n",
      "     BatchNorm2d-318            [-1, 151, 3, 3]             302\n",
      "            ReLU-319            [-1, 151, 3, 3]               0\n",
      "          Conv2d-320             [-1, 48, 3, 3]           7,248\n",
      "    bn_relu_conv-321             [-1, 48, 3, 3]               0\n",
      "     BatchNorm2d-322             [-1, 48, 3, 3]              96\n",
      "            ReLU-323             [-1, 48, 3, 3]               0\n",
      "          Conv2d-324             [-1, 12, 3, 3]           5,184\n",
      "    bn_relu_conv-325             [-1, 12, 3, 3]               0\n",
      "     BatchNorm2d-326            [-1, 163, 3, 3]             326\n",
      "            ReLU-327            [-1, 163, 3, 3]               0\n",
      "          Conv2d-328             [-1, 48, 3, 3]           7,824\n",
      "    bn_relu_conv-329             [-1, 48, 3, 3]               0\n",
      "     BatchNorm2d-330             [-1, 48, 3, 3]              96\n",
      "            ReLU-331             [-1, 48, 3, 3]               0\n",
      "          Conv2d-332             [-1, 12, 3, 3]           5,184\n",
      "    bn_relu_conv-333             [-1, 12, 3, 3]               0\n",
      "     BatchNorm2d-334            [-1, 175, 3, 3]             350\n",
      "            ReLU-335            [-1, 175, 3, 3]               0\n",
      "          Conv2d-336             [-1, 48, 3, 3]           8,400\n",
      "    bn_relu_conv-337             [-1, 48, 3, 3]               0\n",
      "     BatchNorm2d-338             [-1, 48, 3, 3]              96\n",
      "            ReLU-339             [-1, 48, 3, 3]               0\n",
      "          Conv2d-340             [-1, 12, 3, 3]           5,184\n",
      "    bn_relu_conv-341             [-1, 12, 3, 3]               0\n",
      "     BatchNorm2d-342            [-1, 187, 3, 3]             374\n",
      "            ReLU-343            [-1, 187, 3, 3]               0\n",
      "          Conv2d-344             [-1, 93, 3, 3]          17,391\n",
      "    bn_relu_conv-345             [-1, 93, 3, 3]               0\n",
      "       MaxPool2d-346             [-1, 93, 2, 2]               0\n",
      "     BatchNorm2d-347             [-1, 93, 2, 2]             186\n",
      "            ReLU-348             [-1, 93, 2, 2]               0\n",
      "          Conv2d-349             [-1, 48, 2, 2]           4,464\n",
      "    bn_relu_conv-350             [-1, 48, 2, 2]               0\n",
      "     BatchNorm2d-351             [-1, 48, 2, 2]              96\n",
      "            ReLU-352             [-1, 48, 2, 2]               0\n",
      "          Conv2d-353             [-1, 12, 2, 2]           5,184\n",
      "    bn_relu_conv-354             [-1, 12, 2, 2]               0\n",
      "     BatchNorm2d-355            [-1, 105, 2, 2]             210\n",
      "            ReLU-356            [-1, 105, 2, 2]               0\n",
      "          Conv2d-357             [-1, 48, 2, 2]           5,040\n",
      "    bn_relu_conv-358             [-1, 48, 2, 2]               0\n",
      "     BatchNorm2d-359             [-1, 48, 2, 2]              96\n",
      "            ReLU-360             [-1, 48, 2, 2]               0\n",
      "          Conv2d-361             [-1, 12, 2, 2]           5,184\n",
      "    bn_relu_conv-362             [-1, 12, 2, 2]               0\n",
      "     BatchNorm2d-363            [-1, 117, 2, 2]             234\n",
      "            ReLU-364            [-1, 117, 2, 2]               0\n",
      "          Conv2d-365             [-1, 48, 2, 2]           5,616\n",
      "    bn_relu_conv-366             [-1, 48, 2, 2]               0\n",
      "     BatchNorm2d-367             [-1, 48, 2, 2]              96\n",
      "            ReLU-368             [-1, 48, 2, 2]               0\n",
      "          Conv2d-369             [-1, 12, 2, 2]           5,184\n",
      "    bn_relu_conv-370             [-1, 12, 2, 2]               0\n",
      "     BatchNorm2d-371            [-1, 129, 2, 2]             258\n",
      "            ReLU-372            [-1, 129, 2, 2]               0\n",
      "          Conv2d-373             [-1, 48, 2, 2]           6,192\n",
      "    bn_relu_conv-374             [-1, 48, 2, 2]               0\n",
      "     BatchNorm2d-375             [-1, 48, 2, 2]              96\n",
      "            ReLU-376             [-1, 48, 2, 2]               0\n",
      "          Conv2d-377             [-1, 12, 2, 2]           5,184\n",
      "    bn_relu_conv-378             [-1, 12, 2, 2]               0\n",
      "     BatchNorm2d-379            [-1, 141, 2, 2]             282\n",
      "            ReLU-380            [-1, 141, 2, 2]               0\n",
      "          Conv2d-381             [-1, 48, 2, 2]           6,768\n",
      "    bn_relu_conv-382             [-1, 48, 2, 2]               0\n",
      "     BatchNorm2d-383             [-1, 48, 2, 2]              96\n",
      "            ReLU-384             [-1, 48, 2, 2]               0\n",
      "          Conv2d-385             [-1, 12, 2, 2]           5,184\n",
      "    bn_relu_conv-386             [-1, 12, 2, 2]               0\n",
      "     BatchNorm2d-387            [-1, 153, 2, 2]             306\n",
      "            ReLU-388            [-1, 153, 2, 2]               0\n",
      "          Conv2d-389             [-1, 48, 2, 2]           7,344\n",
      "    bn_relu_conv-390             [-1, 48, 2, 2]               0\n",
      "     BatchNorm2d-391             [-1, 48, 2, 2]              96\n",
      "            ReLU-392             [-1, 48, 2, 2]               0\n",
      "          Conv2d-393             [-1, 12, 2, 2]           5,184\n",
      "    bn_relu_conv-394             [-1, 12, 2, 2]               0\n",
      "     BatchNorm2d-395            [-1, 165, 2, 2]             330\n",
      "            ReLU-396            [-1, 165, 2, 2]               0\n",
      "          Conv2d-397             [-1, 48, 2, 2]           7,920\n",
      "    bn_relu_conv-398             [-1, 48, 2, 2]               0\n",
      "     BatchNorm2d-399             [-1, 48, 2, 2]              96\n",
      "            ReLU-400             [-1, 48, 2, 2]               0\n",
      "          Conv2d-401             [-1, 12, 2, 2]           5,184\n",
      "    bn_relu_conv-402             [-1, 12, 2, 2]               0\n",
      "     BatchNorm2d-403            [-1, 177, 2, 2]             354\n",
      "            ReLU-404            [-1, 177, 2, 2]               0\n",
      "          Conv2d-405             [-1, 48, 2, 2]           8,496\n",
      "    bn_relu_conv-406             [-1, 48, 2, 2]               0\n",
      "     BatchNorm2d-407             [-1, 48, 2, 2]              96\n",
      "            ReLU-408             [-1, 48, 2, 2]               0\n",
      "          Conv2d-409             [-1, 12, 2, 2]           5,184\n",
      "    bn_relu_conv-410             [-1, 12, 2, 2]               0\n",
      "          Linear-411                   [-1, 10]           1,900\n",
      "================================================================\n",
      "Total params: 600,202\n",
      "Trainable params: 600,202\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 37.22\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 39.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  3200/50000] loss: 2.0742316\n",
      "[1,  6400/50000] loss: 1.9434163\n",
      "[1,  9600/50000] loss: 1.8733553\n",
      "[1, 12800/50000] loss: 1.8360971\n",
      "[1, 16000/50000] loss: 1.7817126\n",
      "[1, 19200/50000] loss: 1.7066810\n",
      "[1, 22400/50000] loss: 1.6241497\n",
      "[1, 25600/50000] loss: 1.5543298\n",
      "[1, 28800/50000] loss: 1.5984785\n",
      "[1, 32000/50000] loss: 1.4897033\n",
      "[1, 35200/50000] loss: 1.4396772\n",
      "[1, 38400/50000] loss: 1.4046926\n",
      "[1, 41600/50000] loss: 1.3573792\n",
      "[1, 44800/50000] loss: 1.3367807\n",
      "[1 epoch] Accuracy of the network on the validation images: 45 %\n",
      "[2,  3200/50000] loss: 1.2822457\n",
      "[2,  6400/50000] loss: 1.3059492\n",
      "[2,  9600/50000] loss: 1.2668387\n",
      "[2, 12800/50000] loss: 1.2406959\n",
      "[2, 16000/50000] loss: 1.1780404\n",
      "[2, 19200/50000] loss: 1.1568788\n",
      "[2, 22400/50000] loss: 1.1879728\n",
      "[2, 25600/50000] loss: 1.1184612\n",
      "[2, 28800/50000] loss: 1.1187523\n",
      "[2, 32000/50000] loss: 1.1075770\n",
      "[2, 35200/50000] loss: 1.1025272\n",
      "[2, 38400/50000] loss: 1.0857208\n",
      "[2, 41600/50000] loss: 1.1437338\n",
      "[2, 44800/50000] loss: 1.1443597\n",
      "[2 epoch] Accuracy of the network on the validation images: 52 %\n",
      "[3,  3200/50000] loss: 1.0779473\n",
      "[3,  6400/50000] loss: 1.0541576\n",
      "[3,  9600/50000] loss: 1.0209595\n",
      "[3, 12800/50000] loss: 1.0227790\n",
      "[3, 16000/50000] loss: 1.0100614\n",
      "[3, 19200/50000] loss: 1.0332571\n",
      "[3, 22400/50000] loss: 0.9933052\n",
      "[3, 25600/50000] loss: 1.0256689\n",
      "[3, 28800/50000] loss: 0.9631526\n",
      "[3, 32000/50000] loss: 0.9893049\n",
      "[3, 35200/50000] loss: 0.9964234\n",
      "[3, 38400/50000] loss: 0.9475706\n",
      "[3, 41600/50000] loss: 0.9297503\n",
      "[3, 44800/50000] loss: 0.9679678\n",
      "[3 epoch] Accuracy of the network on the validation images: 62 %\n",
      "[4,  3200/50000] loss: 0.9329935\n",
      "[4,  6400/50000] loss: 0.9014351\n",
      "[4,  9600/50000] loss: 0.8743730\n",
      "[4, 12800/50000] loss: 0.9275372\n",
      "[4, 16000/50000] loss: 0.9504817\n",
      "[4, 19200/50000] loss: 0.9012518\n",
      "[4, 22400/50000] loss: 0.8666996\n",
      "[4, 25600/50000] loss: 0.8562778\n",
      "[4, 28800/50000] loss: 0.8722546\n",
      "[4, 32000/50000] loss: 0.8841751\n",
      "[4, 35200/50000] loss: 0.8680072\n",
      "[4, 38400/50000] loss: 0.8710921\n",
      "[4, 41600/50000] loss: 0.8332057\n",
      "[4, 44800/50000] loss: 0.8600508\n",
      "[4 epoch] Accuracy of the network on the validation images: 66 %\n",
      "[5,  3200/50000] loss: 0.8020809\n",
      "[5,  6400/50000] loss: 0.8057530\n",
      "[5,  9600/50000] loss: 0.8216499\n",
      "[5, 12800/50000] loss: 0.7583741\n",
      "[5, 16000/50000] loss: 0.8214044\n",
      "[5, 19200/50000] loss: 0.8519134\n",
      "[5, 22400/50000] loss: 0.7679085\n",
      "[5, 25600/50000] loss: 0.8181342\n",
      "[5, 28800/50000] loss: 0.7872401\n",
      "[5, 32000/50000] loss: 0.7830725\n",
      "[5, 35200/50000] loss: 0.7729653\n",
      "[5, 38400/50000] loss: 0.7754897\n",
      "[5, 41600/50000] loss: 0.8035142\n",
      "[5, 44800/50000] loss: 0.7515108\n",
      "[5 epoch] Accuracy of the network on the validation images: 70 %\n",
      "[6,  3200/50000] loss: 0.6251168\n",
      "[6,  6400/50000] loss: 0.6171734\n",
      "[6,  9600/50000] loss: 0.5709087\n",
      "[6, 12800/50000] loss: 0.5863987\n",
      "[6, 16000/50000] loss: 0.5816521\n",
      "[6, 19200/50000] loss: 0.5521857\n",
      "[6, 22400/50000] loss: 0.5853689\n",
      "[6, 25600/50000] loss: 0.5530900\n",
      "[6, 28800/50000] loss: 0.5530023\n",
      "[6, 32000/50000] loss: 0.5712613\n",
      "[6, 35200/50000] loss: 0.5515319\n",
      "[6, 38400/50000] loss: 0.5639543\n",
      "[6, 41600/50000] loss: 0.5805594\n",
      "[6, 44800/50000] loss: 0.5308488\n",
      "[6 epoch] Accuracy of the network on the validation images: 76 %\n",
      "[7,  3200/50000] loss: 0.5239615\n",
      "[7,  6400/50000] loss: 0.5401291\n",
      "[7,  9600/50000] loss: 0.5335229\n",
      "[7, 12800/50000] loss: 0.5738134\n",
      "[7, 16000/50000] loss: 0.5224630\n",
      "[7, 19200/50000] loss: 0.5284576\n",
      "[7, 22400/50000] loss: 0.5280339\n",
      "[7, 25600/50000] loss: 0.5422520\n",
      "[7, 28800/50000] loss: 0.5137639\n",
      "[7, 32000/50000] loss: 0.5437797\n",
      "[7, 35200/50000] loss: 0.5348259\n",
      "[7, 38400/50000] loss: 0.5176248\n",
      "[7, 41600/50000] loss: 0.5100200\n",
      "[7, 44800/50000] loss: 0.5406058\n",
      "[7 epoch] Accuracy of the network on the validation images: 76 %\n",
      "[8,  3200/50000] loss: 0.5036534\n",
      "[8,  6400/50000] loss: 0.4974966\n",
      "[8,  9600/50000] loss: 0.5218564\n",
      "[8, 12800/50000] loss: 0.5003047\n",
      "[8, 16000/50000] loss: 0.5101070\n",
      "[8, 19200/50000] loss: 0.4935137\n",
      "[8, 22400/50000] loss: 0.4901623\n",
      "[8, 25600/50000] loss: 0.5190021\n",
      "[8, 28800/50000] loss: 0.4802699\n",
      "[8, 32000/50000] loss: 0.4730642\n",
      "[8, 35200/50000] loss: 0.5016257\n",
      "[8, 38400/50000] loss: 0.4986526\n",
      "[8, 41600/50000] loss: 0.5032738\n",
      "[8, 44800/50000] loss: 0.4909834\n",
      "[8 epoch] Accuracy of the network on the validation images: 77 %\n",
      "[9,  3200/50000] loss: 0.5002353\n",
      "[9,  6400/50000] loss: 0.5002312\n",
      "[9,  9600/50000] loss: 0.4786937\n",
      "[9, 12800/50000] loss: 0.5200159\n",
      "[9, 16000/50000] loss: 0.4897897\n",
      "[9, 19200/50000] loss: 0.4769213\n",
      "[9, 22400/50000] loss: 0.4579826\n",
      "[9, 25600/50000] loss: 0.5003235\n",
      "[9, 28800/50000] loss: 0.4829412\n",
      "[9, 32000/50000] loss: 0.4838316\n",
      "[9, 35200/50000] loss: 0.4948856\n",
      "[9, 38400/50000] loss: 0.5037073\n",
      "[9, 41600/50000] loss: 0.4663961\n",
      "[9, 44800/50000] loss: 0.4862187\n",
      "[9 epoch] Accuracy of the network on the validation images: 77 %\n",
      "[10,  3200/50000] loss: 0.5010352\n",
      "[10,  6400/50000] loss: 0.4861401\n",
      "[10,  9600/50000] loss: 0.4747900\n",
      "[10, 12800/50000] loss: 0.5038612\n",
      "[10, 16000/50000] loss: 0.4919029\n",
      "[10, 19200/50000] loss: 0.4859693\n",
      "[10, 22400/50000] loss: 0.4965458\n",
      "[10, 25600/50000] loss: 0.4876345\n",
      "[10, 28800/50000] loss: 0.4729154\n",
      "[10, 32000/50000] loss: 0.4551304\n",
      "[10, 35200/50000] loss: 0.4772405\n",
      "[10, 38400/50000] loss: 0.4952617\n",
      "[10, 41600/50000] loss: 0.4886064\n",
      "[10, 44800/50000] loss: 0.4631113\n",
      "[10 epoch] Accuracy of the network on the validation images: 77 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9)\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[int(num_epoch * 0.5), int(num_epoch * 0.75)], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "for epoch in range(num_epoch):  \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        show_period = 100\n",
    "        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n",
    "            print('[%d, %5d/50000] loss: %.7f' %\n",
    "                  (epoch + 1, (i + 1)*batch_size, running_loss / show_period))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        \n",
    "    # validation part\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(valid_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('[%d epoch] Accuracy of the network on the validation images: %d %%' % \n",
    "          (epoch + 1, 100 * correct / total)\n",
    "         )\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
