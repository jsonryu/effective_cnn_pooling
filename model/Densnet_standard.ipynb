{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"Densnet_total.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"x_0q1HRDz6Fw"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import os\n","import glob\n","import PIL\n","from PIL import Image\n","from torch.utils import data as D\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import random\n","import torchsummary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QOA48SN_z6GA","outputId":"0f503920-5f0f-4783-da62-6d0e1932a867"},"source":["print(torch.__version__)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1.0.0\n","cuda:0\n"]}]},{"cell_type":"code","metadata":{"id":"4yvhmwhbz6GE"},"source":["batch_size = 64\n","validation_ratio = 0.1\n","random_seed = 10\n","initial_lr = 0.1\n","num_epoch = 300\n","# ptype -> 'max', 'avg', 'gauss_HWCN', 'gauss_CN', 'gauss_half_HWCN', 'gauss_half_CN'\n","ptype = 'gauss_half_HWCN'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjpSOrwez6GG","outputId":"ffdd30b1-563a-4fc9-bb30-962049419e55"},"source":["transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","transform_validation = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","\n","transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train)\n","\n","validset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_validation)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test)\n","\n","\n","num_train = len(trainset)\n","indices = list(range(num_train))\n","split = int(np.floor(validation_ratio * num_train))\n","\n","np.random.seed(random_seed)\n","np.random.shuffle(indices)\n","\n","train_idx, valid_idx = indices[split:], indices[:split]\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_loader = torch.utils.data.DataLoader(\n","    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=0\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    testset, batch_size=batch_size, shuffle=False, num_workers=0\n",")\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","metadata":{"id":"ogdA4Ic1z6GL"},"source":["class bn_relu_conv(nn.Module):\n","    def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n","        super(bn_relu_conv, self).__init__()\n","        self.batch_norm = nn.BatchNorm2d(nin)\n","        self.relu = nn.ReLU(True)\n","        self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n","\n","    def forward(self, x):\n","        out = self.batch_norm(x)\n","        out = self.relu(out)\n","        out = self.conv(out)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FfVrtcd3z6GN"},"source":["class bottleneck_layer(nn.Sequential):\n","  def __init__(self, nin, growth_rate, drop_rate=0.2):    \n","      super(bottleneck_layer, self).__init__()\n","      \n","      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n","      self.add_module('conv_3x3', bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n","      \n","      self.drop_rate = drop_rate\n","      \n","  def forward(self, x):\n","      bottleneck_output = super(bottleneck_layer, self).forward(x)\n","      if self.drop_rate > 0:\n","          bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n","          \n","      bottleneck_output = torch.cat((x, bottleneck_output), 1)\n","      \n","      return bottleneck_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNOXzYkBz6GR"},"source":["class GaussianPooling2d(nn.AvgPool2d):\n","    def __init__(self, num_features, kernel_size, stride=None, padding=0, ceil_mode=False,\n","                 count_include_pad=True, hidden_node=None, stochasticity='HWCN', eps=1e-6):\n","        if stochasticity != 'HWCN' and stochasticity != 'CN' and stochasticity is not None:\n","            raise ValueError(\"gaussian pooling stochasticity has to be 'HWCN'/'CN' or None, \"\n","                         \"but got {}\".format(stochasticity))\n","        if hidden_node is None:\n","            hidden_node = num_features // 2\n","\n","        super(GaussianPooling2d, self).__init__(kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode,\n","                    count_include_pad=count_include_pad)\n","        self.eps = eps\n","        self.stochasticity = stochasticity\n","\n","        self.ToHidden = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Conv2d(num_features, hidden_node, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(hidden_node),\n","            nn.ReLU(False),\n","        )\n","        self.ToMean = nn.Sequential(\n","            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(num_features),\n","        )\n","        self.ToSigma = nn.Sequential(\n","            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(num_features),\n","            nn.Sigmoid()\n","        )\n","        self.activation = nn.Softplus()\n","        \n","    def forward(self, input):\n","        mu0 = F.avg_pool2d(input, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n","        sig0= F.avg_pool2d(input**2, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n","        sig0= torch.sqrt(torch.clamp(sig0 - mu0**2, self.eps))\n","\n","        Z = self.ToHidden(input)\n","        MU = self.ToMean(Z)\n","\n","        if self.training and self.stochasticity is not None:\n","            SIGMA = self.ToSigma(Z)\n","            if self.stochasticity == 'HWCN':\n","                size = sig0.size()\n","            else:\n","                size = [sig0.size(0), sig0.size(1), 1, 1]\n","            W = self.activation(MU + SIGMA * \n","                torch.randn(size, dtype=sig0.dtype, layout=sig0.layout, device=sig0.device))\n","        else:\n","            W = self.activation(MU)\n","\n","        return mu0 + W*sig0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yq5Umn8hz6GT"},"source":["class halfGaussianPooling2d(nn.AvgPool2d):\n","    def __init__(self, num_features, kernel_size, stride=None, padding=0, ceil_mode=False,\n","                 count_include_pad=True, hidden_node=None, stochasticity='HWCN', eps=1e-6):\n","        if stochasticity != 'HWCN' and stochasticity != 'CN' and stochasticity is not None:\n","            raise ValueError(\"gaussian pooling stochasticity has to be 'HWCN'/'CN' or None, \"\n","                         \"but got {}\".format(stochasticity))\n","        if hidden_node is None:\n","            hidden_node = num_features // 2\n","\n","        super(halfGaussianPooling2d, self).__init__(kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode,\n","                    count_include_pad=count_include_pad)\n","        self.eps = eps\n","        self.stochasticity = stochasticity\n","\n","        self.ToHidden = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Conv2d(num_features, hidden_node, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(hidden_node),\n","            nn.ReLU(False),\n","        )\n","        self.ToSigma = nn.Sequential(\n","            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(num_features),\n","            nn.Softplus()\n","        )\n","        \n","    def forward(self, input):\n","        mu0 = F.avg_pool2d(input, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n","        sig0= F.avg_pool2d(input**2, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n","        sig0= torch.sqrt(torch.clamp(sig0 - mu0**2, self.eps))\n","\n","        Z = self.ToHidden(input)\n","        SIGMA = self.ToSigma(Z)\n","        \n","        if self.training and self.stochasticity is not None:\n","            if self.stochasticity == 'HWCN':\n","                size = sig0.size()\n","            else:\n","                size = [sig0.size(0), sig0.size(1), 1, 1]\n","            W = torch.abs(torch.randn(size, dtype=sig0.dtype, layout=sig0.layout, device=sig0.device)) * SIGMA\n","        else:\n","            W = (math.sqrt(2) / math.sqrt(math.pi)) * SIGMA\n","\n","        return mu0 + W*sig0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gye1Ua9Yz6GV"},"source":["def _pooling(ptype, num_features, kernel_size, stride, padding=0):\n","    if ptype == 'max':\n","        pool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n","    elif ptype == 'avg':\n","        pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n","    elif ptype == 'gauss_HWCN':\n","        pool = GaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding)\n","    elif ptype == 'gauss_CN':\n","        pool = GaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding, stochasticity='CN')\n","    elif ptype == 'gauss_half_HWCN':\n","        pool = halfGaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding)\n","    elif ptype == 'gauss_half_CN':\n","        pool = halfGaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding, stochasticity='CN')\n","    else:\n","        raise ValueError(\"pooling type of {} is not supported\".format(ptype))\n","    return pool"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-lMHapPWz6GX"},"source":["class Transition_layer(nn.Sequential):\n","  def __init__(self, nin, theta=0.5):    \n","      super(Transition_layer, self).__init__()\n","      \n","      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n","      self.add_module('{}'.format(ptype), _pooling(ptype=ptype, num_features=int(nin*theta), kernel_size=2, stride=2, padding=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LQ4IBnrz6GY"},"source":["class DenseBlock(nn.Sequential):\n","  def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n","      super(DenseBlock, self).__init__()\n","                        \n","      for i in range(num_bottleneck_layers):\n","          nin_bottleneck_layer = nin + growth_rate * i\n","          self.add_module('bottleneck_layer_%d' % i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FgeQU6Ivz6GZ"},"source":["class DenseNet(nn.Module):\n","    def __init__(self, growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10):\n","        super(DenseNet, self).__init__()\n","        \n","        assert (num_layers - 4) % 6 == 0\n","        \n","        # (num_layers-4)//6 \n","        num_bottleneck_layers = (num_layers - 4) // 6\n","        \n","        # 32 x 32 x 3 --> 32 x 32 x (growth_rate*2)\n","        self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=3, stride=1, padding=1, bias=True)\n","                \n","        # 32 x 32 x (growth_rate*2) --> 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","\n","        # 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)] --> 16 x 16 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n","        self.transition_layer_1 = Transition_layer(nin=nin_transition_layer_1, theta=theta)\n","        \n","        # 16 x 16 x nin_transition_layer_1*theta --> 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","\n","        # 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)] --> 8 x 8 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n","        self.transition_layer_2 = Transition_layer(nin=nin_transition_layer_2, theta=theta)\n","        \n","        # 8 x 8 x nin_transition_layer_2*theta --> 8 x 8 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","        \n","        nin_fc_layer = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers) \n","        \n","        # [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> num_classes\n","        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n","        \n","    def forward(self, x):\n","        dense_init_output = self.dense_init(x)\n","        \n","        dense_block_1_output = self.dense_block_1(dense_init_output)\n","        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n","        \n","        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n","        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n","        \n","        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n","        \n","        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_3_output, (1, 1))                \n","        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n","\n","        output = self.fc_layer(global_avg_pool_output_flat)\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyeQG2E8z6Gb"},"source":["def DenseNetBC_100_12():\n","    return DenseNet(growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10)\n","\n","def DenseNetBC_250_24():\n","    return DenseNet(growth_rate=24, num_layers=250, theta=0.5, drop_rate=0.2, num_classes=10)\n","\n","def DenseNetBC_190_40():\n","    return DenseNet(growth_rate=40, num_layers=190, theta=0.5, drop_rate=0.2, num_classes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AlOZr49z6Gc"},"source":["net = DenseNetBC_100_12()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1kzzfTRz6Gf","outputId":"8f374ffe-464f-4722-b08e-13bec29db1a2"},"source":["net.to(device)"],"execution_count":null,"outputs":[{"data":{"text/plain":["DenseNet(\n","  (dense_init): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (dense_block_1): DenseBlock(\n","    (bottleneck_layer_0): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_1): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_2): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_3): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_4): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_5): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_6): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_7): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_8): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_9): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_10): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_11): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_12): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_13): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_14): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_15): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","  )\n","  (transition_layer_1): Transition_layer(\n","    (conv_1x1): bn_relu_conv(\n","      (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace)\n","      (conv): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    )\n","    (gauss_half_HWCN): halfGaussianPooling2d(\n","      kernel_size=2, stride=2, padding=0\n","      (ToHidden): Sequential(\n","        (0): AdaptiveAvgPool2d(output_size=(1, 1))\n","        (1): Conv2d(108, 54, kernel_size=(1, 1), stride=(1, 1))\n","        (2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (3): ReLU()\n","      )\n","      (ToSigma): Sequential(\n","        (0): Conv2d(54, 108, kernel_size=(1, 1), stride=(1, 1))\n","        (1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): Softplus(beta=1, threshold=20)\n","      )\n","    )\n","  )\n","  (dense_block_2): DenseBlock(\n","    (bottleneck_layer_0): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_1): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_2): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_3): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_4): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_5): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_6): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_7): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_8): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_9): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_10): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_11): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_12): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_13): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_14): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_15): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","  )\n","  (transition_layer_2): Transition_layer(\n","    (conv_1x1): bn_relu_conv(\n","      (batch_norm): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace)\n","      (conv): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    )\n","    (gauss_half_HWCN): halfGaussianPooling2d(\n","      kernel_size=2, stride=2, padding=0\n","      (ToHidden): Sequential(\n","        (0): AdaptiveAvgPool2d(output_size=(1, 1))\n","        (1): Conv2d(150, 75, kernel_size=(1, 1), stride=(1, 1))\n","        (2): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (3): ReLU()\n","      )\n","      (ToSigma): Sequential(\n","        (0): Conv2d(75, 150, kernel_size=(1, 1), stride=(1, 1))\n","        (1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): Softplus(beta=1, threshold=20)\n","      )\n","    )\n","  )\n","  (dense_block_3): DenseBlock(\n","    (bottleneck_layer_0): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_1): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_2): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_3): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_4): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_5): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_6): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_7): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_8): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_9): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_10): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_11): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_12): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_13): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_14): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (bottleneck_layer_15): bottleneck_layer(\n","      (conv_1x1): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (conv_3x3): bn_relu_conv(\n","        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace)\n","        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","  )\n","  (fc_layer): Linear(in_features=342, out_features=10, bias=True)\n",")"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"MvsRdldPz6Gg","outputId":"11a09512-4838-464c-92df-607bba170d17"},"source":["torchsummary.summary(net, (3, 32, 32))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 24, 32, 32]             672\n","       BatchNorm2d-2           [-1, 24, 32, 32]              48\n","              ReLU-3           [-1, 24, 32, 32]               0\n","            Conv2d-4           [-1, 48, 32, 32]           1,152\n","      bn_relu_conv-5           [-1, 48, 32, 32]               0\n","       BatchNorm2d-6           [-1, 48, 32, 32]              96\n","              ReLU-7           [-1, 48, 32, 32]               0\n","            Conv2d-8           [-1, 12, 32, 32]           5,184\n","      bn_relu_conv-9           [-1, 12, 32, 32]               0\n","      BatchNorm2d-10           [-1, 36, 32, 32]              72\n","             ReLU-11           [-1, 36, 32, 32]               0\n","           Conv2d-12           [-1, 48, 32, 32]           1,728\n","     bn_relu_conv-13           [-1, 48, 32, 32]               0\n","      BatchNorm2d-14           [-1, 48, 32, 32]              96\n","             ReLU-15           [-1, 48, 32, 32]               0\n","           Conv2d-16           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-17           [-1, 12, 32, 32]               0\n","      BatchNorm2d-18           [-1, 48, 32, 32]              96\n","             ReLU-19           [-1, 48, 32, 32]               0\n","           Conv2d-20           [-1, 48, 32, 32]           2,304\n","     bn_relu_conv-21           [-1, 48, 32, 32]               0\n","      BatchNorm2d-22           [-1, 48, 32, 32]              96\n","             ReLU-23           [-1, 48, 32, 32]               0\n","           Conv2d-24           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-25           [-1, 12, 32, 32]               0\n","      BatchNorm2d-26           [-1, 60, 32, 32]             120\n","             ReLU-27           [-1, 60, 32, 32]               0\n","           Conv2d-28           [-1, 48, 32, 32]           2,880\n","     bn_relu_conv-29           [-1, 48, 32, 32]               0\n","      BatchNorm2d-30           [-1, 48, 32, 32]              96\n","             ReLU-31           [-1, 48, 32, 32]               0\n","           Conv2d-32           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-33           [-1, 12, 32, 32]               0\n","      BatchNorm2d-34           [-1, 72, 32, 32]             144\n","             ReLU-35           [-1, 72, 32, 32]               0\n","           Conv2d-36           [-1, 48, 32, 32]           3,456\n","     bn_relu_conv-37           [-1, 48, 32, 32]               0\n","      BatchNorm2d-38           [-1, 48, 32, 32]              96\n","             ReLU-39           [-1, 48, 32, 32]               0\n","           Conv2d-40           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-41           [-1, 12, 32, 32]               0\n","      BatchNorm2d-42           [-1, 84, 32, 32]             168\n","             ReLU-43           [-1, 84, 32, 32]               0\n","           Conv2d-44           [-1, 48, 32, 32]           4,032\n","     bn_relu_conv-45           [-1, 48, 32, 32]               0\n","      BatchNorm2d-46           [-1, 48, 32, 32]              96\n","             ReLU-47           [-1, 48, 32, 32]               0\n","           Conv2d-48           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-49           [-1, 12, 32, 32]               0\n","      BatchNorm2d-50           [-1, 96, 32, 32]             192\n","             ReLU-51           [-1, 96, 32, 32]               0\n","           Conv2d-52           [-1, 48, 32, 32]           4,608\n","     bn_relu_conv-53           [-1, 48, 32, 32]               0\n","      BatchNorm2d-54           [-1, 48, 32, 32]              96\n","             ReLU-55           [-1, 48, 32, 32]               0\n","           Conv2d-56           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-57           [-1, 12, 32, 32]               0\n","      BatchNorm2d-58          [-1, 108, 32, 32]             216\n","             ReLU-59          [-1, 108, 32, 32]               0\n","           Conv2d-60           [-1, 48, 32, 32]           5,184\n","     bn_relu_conv-61           [-1, 48, 32, 32]               0\n","      BatchNorm2d-62           [-1, 48, 32, 32]              96\n","             ReLU-63           [-1, 48, 32, 32]               0\n","           Conv2d-64           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-65           [-1, 12, 32, 32]               0\n","      BatchNorm2d-66          [-1, 120, 32, 32]             240\n","             ReLU-67          [-1, 120, 32, 32]               0\n","           Conv2d-68           [-1, 48, 32, 32]           5,760\n","     bn_relu_conv-69           [-1, 48, 32, 32]               0\n","      BatchNorm2d-70           [-1, 48, 32, 32]              96\n","             ReLU-71           [-1, 48, 32, 32]               0\n","           Conv2d-72           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-73           [-1, 12, 32, 32]               0\n","      BatchNorm2d-74          [-1, 132, 32, 32]             264\n","             ReLU-75          [-1, 132, 32, 32]               0\n","           Conv2d-76           [-1, 48, 32, 32]           6,336\n","     bn_relu_conv-77           [-1, 48, 32, 32]               0\n","      BatchNorm2d-78           [-1, 48, 32, 32]              96\n","             ReLU-79           [-1, 48, 32, 32]               0\n","           Conv2d-80           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-81           [-1, 12, 32, 32]               0\n","      BatchNorm2d-82          [-1, 144, 32, 32]             288\n","             ReLU-83          [-1, 144, 32, 32]               0\n","           Conv2d-84           [-1, 48, 32, 32]           6,912\n","     bn_relu_conv-85           [-1, 48, 32, 32]               0\n","      BatchNorm2d-86           [-1, 48, 32, 32]              96\n","             ReLU-87           [-1, 48, 32, 32]               0\n","           Conv2d-88           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-89           [-1, 12, 32, 32]               0\n","      BatchNorm2d-90          [-1, 156, 32, 32]             312\n","             ReLU-91          [-1, 156, 32, 32]               0\n","           Conv2d-92           [-1, 48, 32, 32]           7,488\n","     bn_relu_conv-93           [-1, 48, 32, 32]               0\n","      BatchNorm2d-94           [-1, 48, 32, 32]              96\n","             ReLU-95           [-1, 48, 32, 32]               0\n","           Conv2d-96           [-1, 12, 32, 32]           5,184\n","     bn_relu_conv-97           [-1, 12, 32, 32]               0\n","      BatchNorm2d-98          [-1, 168, 32, 32]             336\n","             ReLU-99          [-1, 168, 32, 32]               0\n","          Conv2d-100           [-1, 48, 32, 32]           8,064\n","    bn_relu_conv-101           [-1, 48, 32, 32]               0\n","     BatchNorm2d-102           [-1, 48, 32, 32]              96\n","            ReLU-103           [-1, 48, 32, 32]               0\n","          Conv2d-104           [-1, 12, 32, 32]           5,184\n","    bn_relu_conv-105           [-1, 12, 32, 32]               0\n","     BatchNorm2d-106          [-1, 180, 32, 32]             360\n","            ReLU-107          [-1, 180, 32, 32]               0\n","          Conv2d-108           [-1, 48, 32, 32]           8,640\n","    bn_relu_conv-109           [-1, 48, 32, 32]               0\n","     BatchNorm2d-110           [-1, 48, 32, 32]              96\n","            ReLU-111           [-1, 48, 32, 32]               0\n","          Conv2d-112           [-1, 12, 32, 32]           5,184\n","    bn_relu_conv-113           [-1, 12, 32, 32]               0\n","     BatchNorm2d-114          [-1, 192, 32, 32]             384\n","            ReLU-115          [-1, 192, 32, 32]               0\n","          Conv2d-116           [-1, 48, 32, 32]           9,216\n","    bn_relu_conv-117           [-1, 48, 32, 32]               0\n","     BatchNorm2d-118           [-1, 48, 32, 32]              96\n","            ReLU-119           [-1, 48, 32, 32]               0\n","          Conv2d-120           [-1, 12, 32, 32]           5,184\n","    bn_relu_conv-121           [-1, 12, 32, 32]               0\n","     BatchNorm2d-122          [-1, 204, 32, 32]             408\n","            ReLU-123          [-1, 204, 32, 32]               0\n","          Conv2d-124           [-1, 48, 32, 32]           9,792\n","    bn_relu_conv-125           [-1, 48, 32, 32]               0\n","     BatchNorm2d-126           [-1, 48, 32, 32]              96\n","            ReLU-127           [-1, 48, 32, 32]               0\n","          Conv2d-128           [-1, 12, 32, 32]           5,184\n","    bn_relu_conv-129           [-1, 12, 32, 32]               0\n","     BatchNorm2d-130          [-1, 216, 32, 32]             432\n","            ReLU-131          [-1, 216, 32, 32]               0\n","          Conv2d-132          [-1, 108, 32, 32]          23,328\n","    bn_relu_conv-133          [-1, 108, 32, 32]               0\n","       MaxPool2d-134          [-1, 108, 16, 16]               0\n","     BatchNorm2d-135          [-1, 108, 16, 16]             216\n","            ReLU-136          [-1, 108, 16, 16]               0\n","          Conv2d-137           [-1, 48, 16, 16]           5,184\n","    bn_relu_conv-138           [-1, 48, 16, 16]               0\n","     BatchNorm2d-139           [-1, 48, 16, 16]              96\n","            ReLU-140           [-1, 48, 16, 16]               0\n","          Conv2d-141           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-142           [-1, 12, 16, 16]               0\n","     BatchNorm2d-143          [-1, 120, 16, 16]             240\n","            ReLU-144          [-1, 120, 16, 16]               0\n","          Conv2d-145           [-1, 48, 16, 16]           5,760\n","    bn_relu_conv-146           [-1, 48, 16, 16]               0\n","     BatchNorm2d-147           [-1, 48, 16, 16]              96\n","            ReLU-148           [-1, 48, 16, 16]               0\n","          Conv2d-149           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-150           [-1, 12, 16, 16]               0\n","     BatchNorm2d-151          [-1, 132, 16, 16]             264\n","            ReLU-152          [-1, 132, 16, 16]               0\n","          Conv2d-153           [-1, 48, 16, 16]           6,336\n","    bn_relu_conv-154           [-1, 48, 16, 16]               0\n","     BatchNorm2d-155           [-1, 48, 16, 16]              96\n","            ReLU-156           [-1, 48, 16, 16]               0\n","          Conv2d-157           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-158           [-1, 12, 16, 16]               0\n","     BatchNorm2d-159          [-1, 144, 16, 16]             288\n","            ReLU-160          [-1, 144, 16, 16]               0\n","          Conv2d-161           [-1, 48, 16, 16]           6,912\n","    bn_relu_conv-162           [-1, 48, 16, 16]               0\n","     BatchNorm2d-163           [-1, 48, 16, 16]              96\n","            ReLU-164           [-1, 48, 16, 16]               0\n","          Conv2d-165           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-166           [-1, 12, 16, 16]               0\n","     BatchNorm2d-167          [-1, 156, 16, 16]             312\n","            ReLU-168          [-1, 156, 16, 16]               0\n","          Conv2d-169           [-1, 48, 16, 16]           7,488\n","    bn_relu_conv-170           [-1, 48, 16, 16]               0\n","     BatchNorm2d-171           [-1, 48, 16, 16]              96\n","            ReLU-172           [-1, 48, 16, 16]               0\n","          Conv2d-173           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-174           [-1, 12, 16, 16]               0\n","     BatchNorm2d-175          [-1, 168, 16, 16]             336\n","            ReLU-176          [-1, 168, 16, 16]               0\n","          Conv2d-177           [-1, 48, 16, 16]           8,064\n","    bn_relu_conv-178           [-1, 48, 16, 16]               0\n","     BatchNorm2d-179           [-1, 48, 16, 16]              96\n","            ReLU-180           [-1, 48, 16, 16]               0\n","          Conv2d-181           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-182           [-1, 12, 16, 16]               0\n","     BatchNorm2d-183          [-1, 180, 16, 16]             360\n","            ReLU-184          [-1, 180, 16, 16]               0\n","          Conv2d-185           [-1, 48, 16, 16]           8,640\n","    bn_relu_conv-186           [-1, 48, 16, 16]               0\n","     BatchNorm2d-187           [-1, 48, 16, 16]              96\n","            ReLU-188           [-1, 48, 16, 16]               0\n","          Conv2d-189           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-190           [-1, 12, 16, 16]               0\n","     BatchNorm2d-191          [-1, 192, 16, 16]             384\n","            ReLU-192          [-1, 192, 16, 16]               0\n","          Conv2d-193           [-1, 48, 16, 16]           9,216\n","    bn_relu_conv-194           [-1, 48, 16, 16]               0\n","     BatchNorm2d-195           [-1, 48, 16, 16]              96\n","            ReLU-196           [-1, 48, 16, 16]               0\n","          Conv2d-197           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-198           [-1, 12, 16, 16]               0\n","     BatchNorm2d-199          [-1, 204, 16, 16]             408\n","            ReLU-200          [-1, 204, 16, 16]               0\n","          Conv2d-201           [-1, 48, 16, 16]           9,792\n","    bn_relu_conv-202           [-1, 48, 16, 16]               0\n","     BatchNorm2d-203           [-1, 48, 16, 16]              96\n","            ReLU-204           [-1, 48, 16, 16]               0\n","          Conv2d-205           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-206           [-1, 12, 16, 16]               0\n","     BatchNorm2d-207          [-1, 216, 16, 16]             432\n","            ReLU-208          [-1, 216, 16, 16]               0\n","          Conv2d-209           [-1, 48, 16, 16]          10,368\n","    bn_relu_conv-210           [-1, 48, 16, 16]               0\n","     BatchNorm2d-211           [-1, 48, 16, 16]              96\n","            ReLU-212           [-1, 48, 16, 16]               0\n","          Conv2d-213           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-214           [-1, 12, 16, 16]               0\n","     BatchNorm2d-215          [-1, 228, 16, 16]             456\n","            ReLU-216          [-1, 228, 16, 16]               0\n","          Conv2d-217           [-1, 48, 16, 16]          10,944\n","    bn_relu_conv-218           [-1, 48, 16, 16]               0\n","     BatchNorm2d-219           [-1, 48, 16, 16]              96\n","            ReLU-220           [-1, 48, 16, 16]               0\n","          Conv2d-221           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-222           [-1, 12, 16, 16]               0\n","     BatchNorm2d-223          [-1, 240, 16, 16]             480\n","            ReLU-224          [-1, 240, 16, 16]               0\n","          Conv2d-225           [-1, 48, 16, 16]          11,520\n","    bn_relu_conv-226           [-1, 48, 16, 16]               0\n","     BatchNorm2d-227           [-1, 48, 16, 16]              96\n","            ReLU-228           [-1, 48, 16, 16]               0\n","          Conv2d-229           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-230           [-1, 12, 16, 16]               0\n","     BatchNorm2d-231          [-1, 252, 16, 16]             504\n","            ReLU-232          [-1, 252, 16, 16]               0\n","          Conv2d-233           [-1, 48, 16, 16]          12,096\n","    bn_relu_conv-234           [-1, 48, 16, 16]               0\n","     BatchNorm2d-235           [-1, 48, 16, 16]              96\n","            ReLU-236           [-1, 48, 16, 16]               0\n","          Conv2d-237           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-238           [-1, 12, 16, 16]               0\n","     BatchNorm2d-239          [-1, 264, 16, 16]             528\n","            ReLU-240          [-1, 264, 16, 16]               0\n","          Conv2d-241           [-1, 48, 16, 16]          12,672\n","    bn_relu_conv-242           [-1, 48, 16, 16]               0\n","     BatchNorm2d-243           [-1, 48, 16, 16]              96\n","            ReLU-244           [-1, 48, 16, 16]               0\n","          Conv2d-245           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-246           [-1, 12, 16, 16]               0\n","     BatchNorm2d-247          [-1, 276, 16, 16]             552\n","            ReLU-248          [-1, 276, 16, 16]               0\n","          Conv2d-249           [-1, 48, 16, 16]          13,248\n","    bn_relu_conv-250           [-1, 48, 16, 16]               0\n","     BatchNorm2d-251           [-1, 48, 16, 16]              96\n","            ReLU-252           [-1, 48, 16, 16]               0\n","          Conv2d-253           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-254           [-1, 12, 16, 16]               0\n","     BatchNorm2d-255          [-1, 288, 16, 16]             576\n","            ReLU-256          [-1, 288, 16, 16]               0\n","          Conv2d-257           [-1, 48, 16, 16]          13,824\n","    bn_relu_conv-258           [-1, 48, 16, 16]               0\n","     BatchNorm2d-259           [-1, 48, 16, 16]              96\n","            ReLU-260           [-1, 48, 16, 16]               0\n","          Conv2d-261           [-1, 12, 16, 16]           5,184\n","    bn_relu_conv-262           [-1, 12, 16, 16]               0\n","     BatchNorm2d-263          [-1, 300, 16, 16]             600\n","            ReLU-264          [-1, 300, 16, 16]               0\n","          Conv2d-265          [-1, 150, 16, 16]          45,000\n","    bn_relu_conv-266          [-1, 150, 16, 16]               0\n","       MaxPool2d-267            [-1, 150, 8, 8]               0\n","     BatchNorm2d-268            [-1, 150, 8, 8]             300\n","            ReLU-269            [-1, 150, 8, 8]               0\n","          Conv2d-270             [-1, 48, 8, 8]           7,200\n","    bn_relu_conv-271             [-1, 48, 8, 8]               0\n","     BatchNorm2d-272             [-1, 48, 8, 8]              96\n","            ReLU-273             [-1, 48, 8, 8]               0\n","          Conv2d-274             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-275             [-1, 12, 8, 8]               0\n","     BatchNorm2d-276            [-1, 162, 8, 8]             324\n","            ReLU-277            [-1, 162, 8, 8]               0\n","          Conv2d-278             [-1, 48, 8, 8]           7,776\n","    bn_relu_conv-279             [-1, 48, 8, 8]               0\n","     BatchNorm2d-280             [-1, 48, 8, 8]              96\n","            ReLU-281             [-1, 48, 8, 8]               0\n","          Conv2d-282             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-283             [-1, 12, 8, 8]               0\n","     BatchNorm2d-284            [-1, 174, 8, 8]             348\n","            ReLU-285            [-1, 174, 8, 8]               0\n","          Conv2d-286             [-1, 48, 8, 8]           8,352\n","    bn_relu_conv-287             [-1, 48, 8, 8]               0\n","     BatchNorm2d-288             [-1, 48, 8, 8]              96\n","            ReLU-289             [-1, 48, 8, 8]               0\n","          Conv2d-290             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-291             [-1, 12, 8, 8]               0\n","     BatchNorm2d-292            [-1, 186, 8, 8]             372\n","            ReLU-293            [-1, 186, 8, 8]               0\n","          Conv2d-294             [-1, 48, 8, 8]           8,928\n","    bn_relu_conv-295             [-1, 48, 8, 8]               0\n","     BatchNorm2d-296             [-1, 48, 8, 8]              96\n","            ReLU-297             [-1, 48, 8, 8]               0\n","          Conv2d-298             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-299             [-1, 12, 8, 8]               0\n","     BatchNorm2d-300            [-1, 198, 8, 8]             396\n","            ReLU-301            [-1, 198, 8, 8]               0\n","          Conv2d-302             [-1, 48, 8, 8]           9,504\n","    bn_relu_conv-303             [-1, 48, 8, 8]               0\n","     BatchNorm2d-304             [-1, 48, 8, 8]              96\n","            ReLU-305             [-1, 48, 8, 8]               0\n","          Conv2d-306             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-307             [-1, 12, 8, 8]               0\n","     BatchNorm2d-308            [-1, 210, 8, 8]             420\n","            ReLU-309            [-1, 210, 8, 8]               0\n","          Conv2d-310             [-1, 48, 8, 8]          10,080\n","    bn_relu_conv-311             [-1, 48, 8, 8]               0\n","     BatchNorm2d-312             [-1, 48, 8, 8]              96\n","            ReLU-313             [-1, 48, 8, 8]               0\n","          Conv2d-314             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-315             [-1, 12, 8, 8]               0\n","     BatchNorm2d-316            [-1, 222, 8, 8]             444\n","            ReLU-317            [-1, 222, 8, 8]               0\n","          Conv2d-318             [-1, 48, 8, 8]          10,656\n","    bn_relu_conv-319             [-1, 48, 8, 8]               0\n","     BatchNorm2d-320             [-1, 48, 8, 8]              96\n","            ReLU-321             [-1, 48, 8, 8]               0\n","          Conv2d-322             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-323             [-1, 12, 8, 8]               0\n","     BatchNorm2d-324            [-1, 234, 8, 8]             468\n","            ReLU-325            [-1, 234, 8, 8]               0\n","          Conv2d-326             [-1, 48, 8, 8]          11,232\n","    bn_relu_conv-327             [-1, 48, 8, 8]               0\n","     BatchNorm2d-328             [-1, 48, 8, 8]              96\n","            ReLU-329             [-1, 48, 8, 8]               0\n","          Conv2d-330             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-331             [-1, 12, 8, 8]               0\n","     BatchNorm2d-332            [-1, 246, 8, 8]             492\n","            ReLU-333            [-1, 246, 8, 8]               0\n","          Conv2d-334             [-1, 48, 8, 8]          11,808\n","    bn_relu_conv-335             [-1, 48, 8, 8]               0\n","     BatchNorm2d-336             [-1, 48, 8, 8]              96\n","            ReLU-337             [-1, 48, 8, 8]               0\n","          Conv2d-338             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-339             [-1, 12, 8, 8]               0\n","     BatchNorm2d-340            [-1, 258, 8, 8]             516\n","            ReLU-341            [-1, 258, 8, 8]               0\n","          Conv2d-342             [-1, 48, 8, 8]          12,384\n","    bn_relu_conv-343             [-1, 48, 8, 8]               0\n","     BatchNorm2d-344             [-1, 48, 8, 8]              96\n","            ReLU-345             [-1, 48, 8, 8]               0\n","          Conv2d-346             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-347             [-1, 12, 8, 8]               0\n","     BatchNorm2d-348            [-1, 270, 8, 8]             540\n","            ReLU-349            [-1, 270, 8, 8]               0\n","          Conv2d-350             [-1, 48, 8, 8]          12,960\n","    bn_relu_conv-351             [-1, 48, 8, 8]               0\n","     BatchNorm2d-352             [-1, 48, 8, 8]              96\n","            ReLU-353             [-1, 48, 8, 8]               0\n","          Conv2d-354             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-355             [-1, 12, 8, 8]               0\n","     BatchNorm2d-356            [-1, 282, 8, 8]             564\n","            ReLU-357            [-1, 282, 8, 8]               0\n","          Conv2d-358             [-1, 48, 8, 8]          13,536\n","    bn_relu_conv-359             [-1, 48, 8, 8]               0\n","     BatchNorm2d-360             [-1, 48, 8, 8]              96\n","            ReLU-361             [-1, 48, 8, 8]               0\n","          Conv2d-362             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-363             [-1, 12, 8, 8]               0\n","     BatchNorm2d-364            [-1, 294, 8, 8]             588\n","            ReLU-365            [-1, 294, 8, 8]               0\n","          Conv2d-366             [-1, 48, 8, 8]          14,112\n","    bn_relu_conv-367             [-1, 48, 8, 8]               0\n","     BatchNorm2d-368             [-1, 48, 8, 8]              96\n","            ReLU-369             [-1, 48, 8, 8]               0\n","          Conv2d-370             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-371             [-1, 12, 8, 8]               0\n","     BatchNorm2d-372            [-1, 306, 8, 8]             612\n","            ReLU-373            [-1, 306, 8, 8]               0\n","          Conv2d-374             [-1, 48, 8, 8]          14,688\n","    bn_relu_conv-375             [-1, 48, 8, 8]               0\n","     BatchNorm2d-376             [-1, 48, 8, 8]              96\n","            ReLU-377             [-1, 48, 8, 8]               0\n","          Conv2d-378             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-379             [-1, 12, 8, 8]               0\n","     BatchNorm2d-380            [-1, 318, 8, 8]             636\n","            ReLU-381            [-1, 318, 8, 8]               0\n","          Conv2d-382             [-1, 48, 8, 8]          15,264\n","    bn_relu_conv-383             [-1, 48, 8, 8]               0\n","     BatchNorm2d-384             [-1, 48, 8, 8]              96\n","            ReLU-385             [-1, 48, 8, 8]               0\n","          Conv2d-386             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-387             [-1, 12, 8, 8]               0\n","     BatchNorm2d-388            [-1, 330, 8, 8]             660\n","            ReLU-389            [-1, 330, 8, 8]               0\n","          Conv2d-390             [-1, 48, 8, 8]          15,840\n","    bn_relu_conv-391             [-1, 48, 8, 8]               0\n","     BatchNorm2d-392             [-1, 48, 8, 8]              96\n","            ReLU-393             [-1, 48, 8, 8]               0\n","          Conv2d-394             [-1, 12, 8, 8]           5,184\n","    bn_relu_conv-395             [-1, 12, 8, 8]               0\n","          Linear-396                   [-1, 10]           3,430\n","================================================================\n","Total params: 768,502\n","Trainable params: 768,502\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 87.35\n","Params size (MB): 2.93\n","Estimated Total Size (MB): 90.30\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"YQvzafV_z6Gi","outputId":"5bf059bf-4492-4a00-bac1-786a0e575d74"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9)\n","lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[int(num_epoch * 0.5), int(num_epoch * 0.75)], gamma=0.1, last_epoch=-1)\n","\n","for epoch in range(num_epoch):  \n","    lr_scheduler.step()\n","    \n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        \n","        show_period = 100\n","        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n","            print('[%d, %5d/50000] loss: %.7f' %\n","                  (epoch + 1, (i + 1)*batch_size, running_loss / show_period))\n","            running_loss = 0.0\n","        \n","        \n","    # validation part\n","    correct = 0\n","    total = 0\n","    for i, data in enumerate(valid_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net(inputs)\n","        \n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        \n","    print('[%d epoch] Accuracy of the network on the validation images: %d %%' % \n","          (epoch + 1, 100 * correct / total)\n","         )\n","    torch.cuda.empty_cache()\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[1,  3200/50000] loss: 2.0742316\n","[1,  6400/50000] loss: 1.9434163\n","[1,  9600/50000] loss: 1.8733553\n","[1, 12800/50000] loss: 1.8360971\n","[1, 16000/50000] loss: 1.7817126\n","[1, 19200/50000] loss: 1.7066810\n","[1, 22400/50000] loss: 1.6241497\n","[1, 25600/50000] loss: 1.5543298\n","[1, 28800/50000] loss: 1.5984785\n","[1, 32000/50000] loss: 1.4897033\n","[1, 35200/50000] loss: 1.4396772\n","[1, 38400/50000] loss: 1.4046926\n","[1, 41600/50000] loss: 1.3573792\n","[1, 44800/50000] loss: 1.3367807\n","[1 epoch] Accuracy of the network on the validation images: 45 %\n","[2,  3200/50000] loss: 1.2822457\n","[2,  6400/50000] loss: 1.3059492\n","[2,  9600/50000] loss: 1.2668387\n","[2, 12800/50000] loss: 1.2406959\n","[2, 16000/50000] loss: 1.1780404\n","[2, 19200/50000] loss: 1.1568788\n","[2, 22400/50000] loss: 1.1879728\n","[2, 25600/50000] loss: 1.1184612\n","[2, 28800/50000] loss: 1.1187523\n","[2, 32000/50000] loss: 1.1075770\n","[2, 35200/50000] loss: 1.1025272\n","[2, 38400/50000] loss: 1.0857208\n","[2, 41600/50000] loss: 1.1437338\n","[2, 44800/50000] loss: 1.1443597\n","[2 epoch] Accuracy of the network on the validation images: 52 %\n","[3,  3200/50000] loss: 1.0779473\n","[3,  6400/50000] loss: 1.0541576\n","[3,  9600/50000] loss: 1.0209595\n","[3, 12800/50000] loss: 1.0227790\n","[3, 16000/50000] loss: 1.0100614\n","[3, 19200/50000] loss: 1.0332571\n","[3, 22400/50000] loss: 0.9933052\n","[3, 25600/50000] loss: 1.0256689\n","[3, 28800/50000] loss: 0.9631526\n","[3, 32000/50000] loss: 0.9893049\n","[3, 35200/50000] loss: 0.9964234\n","[3, 38400/50000] loss: 0.9475706\n","[3, 41600/50000] loss: 0.9297503\n","[3, 44800/50000] loss: 0.9679678\n","[3 epoch] Accuracy of the network on the validation images: 62 %\n","[4,  3200/50000] loss: 0.9329935\n","[4,  6400/50000] loss: 0.9014351\n","[4,  9600/50000] loss: 0.8743730\n","[4, 12800/50000] loss: 0.9275372\n","[4, 16000/50000] loss: 0.9504817\n","[4, 19200/50000] loss: 0.9012518\n","[4, 22400/50000] loss: 0.8666996\n","[4, 25600/50000] loss: 0.8562778\n","[4, 28800/50000] loss: 0.8722546\n","[4, 32000/50000] loss: 0.8841751\n","[4, 35200/50000] loss: 0.8680072\n","[4, 38400/50000] loss: 0.8710921\n","[4, 41600/50000] loss: 0.8332057\n","[4, 44800/50000] loss: 0.8600508\n","[4 epoch] Accuracy of the network on the validation images: 66 %\n","[5,  3200/50000] loss: 0.8020809\n","[5,  6400/50000] loss: 0.8057530\n","[5,  9600/50000] loss: 0.8216499\n","[5, 12800/50000] loss: 0.7583741\n","[5, 16000/50000] loss: 0.8214044\n","[5, 19200/50000] loss: 0.8519134\n","[5, 22400/50000] loss: 0.7679085\n","[5, 25600/50000] loss: 0.8181342\n","[5, 28800/50000] loss: 0.7872401\n","[5, 32000/50000] loss: 0.7830725\n","[5, 35200/50000] loss: 0.7729653\n","[5, 38400/50000] loss: 0.7754897\n","[5, 41600/50000] loss: 0.8035142\n","[5, 44800/50000] loss: 0.7515108\n","[5 epoch] Accuracy of the network on the validation images: 70 %\n","[6,  3200/50000] loss: 0.6251168\n","[6,  6400/50000] loss: 0.6171734\n","[6,  9600/50000] loss: 0.5709087\n","[6, 12800/50000] loss: 0.5863987\n","[6, 16000/50000] loss: 0.5816521\n","[6, 19200/50000] loss: 0.5521857\n","[6, 22400/50000] loss: 0.5853689\n","[6, 25600/50000] loss: 0.5530900\n","[6, 28800/50000] loss: 0.5530023\n","[6, 32000/50000] loss: 0.5712613\n","[6, 35200/50000] loss: 0.5515319\n","[6, 38400/50000] loss: 0.5639543\n","[6, 41600/50000] loss: 0.5805594\n","[6, 44800/50000] loss: 0.5308488\n","[6 epoch] Accuracy of the network on the validation images: 76 %\n","[7,  3200/50000] loss: 0.5239615\n","[7,  6400/50000] loss: 0.5401291\n","[7,  9600/50000] loss: 0.5335229\n","[7, 12800/50000] loss: 0.5738134\n","[7, 16000/50000] loss: 0.5224630\n","[7, 19200/50000] loss: 0.5284576\n","[7, 22400/50000] loss: 0.5280339\n","[7, 25600/50000] loss: 0.5422520\n","[7, 28800/50000] loss: 0.5137639\n","[7, 32000/50000] loss: 0.5437797\n","[7, 35200/50000] loss: 0.5348259\n","[7, 38400/50000] loss: 0.5176248\n","[7, 41600/50000] loss: 0.5100200\n","[7, 44800/50000] loss: 0.5406058\n","[7 epoch] Accuracy of the network on the validation images: 76 %\n","[8,  3200/50000] loss: 0.5036534\n","[8,  6400/50000] loss: 0.4974966\n","[8,  9600/50000] loss: 0.5218564\n","[8, 12800/50000] loss: 0.5003047\n","[8, 16000/50000] loss: 0.5101070\n","[8, 19200/50000] loss: 0.4935137\n","[8, 22400/50000] loss: 0.4901623\n","[8, 25600/50000] loss: 0.5190021\n","[8, 28800/50000] loss: 0.4802699\n","[8, 32000/50000] loss: 0.4730642\n","[8, 35200/50000] loss: 0.5016257\n","[8, 38400/50000] loss: 0.4986526\n","[8, 41600/50000] loss: 0.5032738\n","[8, 44800/50000] loss: 0.4909834\n","[8 epoch] Accuracy of the network on the validation images: 77 %\n","[9,  3200/50000] loss: 0.5002353\n","[9,  6400/50000] loss: 0.5002312\n","[9,  9600/50000] loss: 0.4786937\n","[9, 12800/50000] loss: 0.5200159\n","[9, 16000/50000] loss: 0.4897897\n","[9, 19200/50000] loss: 0.4769213\n","[9, 22400/50000] loss: 0.4579826\n","[9, 25600/50000] loss: 0.5003235\n","[9, 28800/50000] loss: 0.4829412\n","[9, 32000/50000] loss: 0.4838316\n","[9, 35200/50000] loss: 0.4948856\n","[9, 38400/50000] loss: 0.5037073\n","[9, 41600/50000] loss: 0.4663961\n","[9, 44800/50000] loss: 0.4862187\n","[9 epoch] Accuracy of the network on the validation images: 77 %\n","[10,  3200/50000] loss: 0.5010352\n","[10,  6400/50000] loss: 0.4861401\n","[10,  9600/50000] loss: 0.4747900\n","[10, 12800/50000] loss: 0.5038612\n","[10, 16000/50000] loss: 0.4919029\n","[10, 19200/50000] loss: 0.4859693\n","[10, 22400/50000] loss: 0.4965458\n","[10, 25600/50000] loss: 0.4876345\n","[10, 28800/50000] loss: 0.4729154\n","[10, 32000/50000] loss: 0.4551304\n","[10, 35200/50000] loss: 0.4772405\n","[10, 38400/50000] loss: 0.4952617\n","[10, 41600/50000] loss: 0.4886064\n","[10, 44800/50000] loss: 0.4631113\n","[10 epoch] Accuracy of the network on the validation images: 77 %\n","Finished Training\n"]}]},{"cell_type":"code","metadata":{"id":"3RJOC-zoz6Gi"},"source":[""],"execution_count":null,"outputs":[]}]}