{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qbDkIwF7Nd37"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import os\n","import glob\n","import PIL\n","from PIL import Image\n","from torch.utils import data as D\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import random\n","import torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E71hJNPTNd4A"},"outputs":[],"source":["print(torch.__version__)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leY89euANd4C"},"outputs":[],"source":["batch_size = 64\n","validation_ratio = 0.1\n","random_seed = 10\n","initial_lr = 0.1\n","num_epoch = 300\n","# ptype -> 'max', 'avg', 'gauss_HWCN', 'gauss_CN', 'gauss_half_HWCN', 'gauss_half_CN'\n","ptype = 'max'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBYr6DktNd4D"},"outputs":[],"source":["transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","transform_validation = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","\n","transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train)\n","\n","validset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_validation)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test)\n","\n","\n","num_train = len(trainset)\n","indices = list(range(num_train))\n","split = int(np.floor(validation_ratio * num_train))\n","\n","np.random.seed(random_seed)\n","np.random.shuffle(indices)\n","\n","train_idx, valid_idx = indices[split:], indices[:split]\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_loader = torch.utils.data.DataLoader(\n","    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=0\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    testset, batch_size=batch_size, shuffle=False, num_workers=0\n",")\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aBpwq6PENd4F"},"outputs":[],"source":["class bn_relu_conv(nn.Module):\n","    def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n","        super(bn_relu_conv, self).__init__()\n","        self.batch_norm = nn.BatchNorm2d(nin)\n","        self.relu = nn.ReLU(True)\n","        self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n","\n","    def forward(self, x):\n","        out = self.batch_norm(x)\n","        out = self.relu(out)\n","        out = self.conv(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qewOaT3fNd4G"},"outputs":[],"source":["class bottleneck_layer(nn.Sequential):\n","  def __init__(self, nin, growth_rate, drop_rate=0.2):    \n","      super(bottleneck_layer, self).__init__()\n","      \n","      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n","      self.add_module('conv_3x3', bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n","      \n","      self.drop_rate = drop_rate\n","      \n","  def forward(self, x):\n","      bottleneck_output = super(bottleneck_layer, self).forward(x)\n","      if self.drop_rate > 0:\n","          bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n","          \n","      bottleneck_output = torch.cat((x, bottleneck_output), 1)\n","      \n","      return bottleneck_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1bp9kxfNd4I"},"outputs":[],"source":["class GaussianPooling2d(nn.AvgPool2d):\n","    def __init__(self, num_features, kernel_size, stride=None, padding=0, ceil_mode=False,\n","                 count_include_pad=True, hidden_node=None, stochasticity='HWCN', eps=1e-6):\n","        if stochasticity != 'HWCN' and stochasticity != 'CN' and stochasticity is not None:\n","            raise ValueError(\"gaussian pooling stochasticity has to be 'HWCN'/'CN' or None, \"\n","                         \"but got {}\".format(stochasticity))\n","        if hidden_node is None:\n","            hidden_node = num_features // 2\n","\n","        super(GaussianPooling2d, self).__init__(kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode,\n","                    count_include_pad=count_include_pad)\n","        self.eps = eps\n","        self.stochasticity = stochasticity\n","\n","        self.ToHidden = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Conv2d(num_features, hidden_node, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(hidden_node),\n","            nn.ReLU(False),\n","        )\n","        self.ToMean = nn.Sequential(\n","            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(num_features),\n","        )\n","        self.ToSigma = nn.Sequential(\n","            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(num_features),\n","            nn.Sigmoid()\n","        )\n","        self.activation = nn.Softplus()\n","        \n","    def forward(self, input):\n","        mu0 = F.avg_pool2d(input, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n","        sig0= F.avg_pool2d(input**2, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n","        sig0= torch.sqrt(torch.clamp(sig0 - mu0**2, self.eps))\n","\n","        Z = self.ToHidden(input)\n","        MU = self.ToMean(Z)\n","\n","        if self.training and self.stochasticity is not None:\n","            SIGMA = self.ToSigma(Z)\n","            if self.stochasticity == 'HWCN':\n","                size = sig0.size()\n","            else:\n","                size = [sig0.size(0), sig0.size(1), 1, 1]\n","            W = self.activation(MU + SIGMA * \n","                torch.randn(size, dtype=sig0.dtype, layout=sig0.layout, device=sig0.device))\n","        else:\n","            W = self.activation(MU)\n","\n","        return mu0 + W*sig0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYgDQB-GNd4J"},"outputs":[],"source":["class halfGaussianPooling2d(nn.AvgPool2d):\n","    def __init__(self, num_features, kernel_size, stride=None, padding=0, ceil_mode=False,\n","                 count_include_pad=True, hidden_node=None, stochasticity='HWCN', eps=1e-6):\n","        if stochasticity != 'HWCN' and stochasticity != 'CN' and stochasticity is not None:\n","            raise ValueError(\"gaussian pooling stochasticity has to be 'HWCN'/'CN' or None, \"\n","                         \"but got {}\".format(stochasticity))\n","        if hidden_node is None:\n","            hidden_node = num_features // 2\n","\n","        super(halfGaussianPooling2d, self).__init__(kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode,\n","                    count_include_pad=count_include_pad)\n","        self.eps = eps\n","        self.stochasticity = stochasticity\n","\n","        self.ToHidden = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Conv2d(num_features, hidden_node, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(hidden_node),\n","            nn.ReLU(False),\n","        )\n","        self.ToSigma = nn.Sequential(\n","            nn.Conv2d(hidden_node, num_features, kernel_size=1,  padding=0, bias=True),\n","            nn.BatchNorm2d(num_features),\n","            nn.Softplus()\n","        )\n","        \n","    def forward(self, input):\n","        mu0 = F.avg_pool2d(input, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n","        sig0= F.avg_pool2d(input**2, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n","        sig0= torch.sqrt(torch.clamp(sig0 - mu0**2, self.eps))\n","\n","        Z = self.ToHidden(input)\n","        SIGMA = self.ToSigma(Z)\n","        \n","        if self.training and self.stochasticity is not None:\n","            if self.stochasticity == 'HWCN':\n","                size = sig0.size()\n","            else:\n","                size = [sig0.size(0), sig0.size(1), 1, 1]\n","            W = torch.abs(torch.randn(size, dtype=sig0.dtype, layout=sig0.layout, device=sig0.device)) * SIGMA\n","        else:\n","            W = (math.sqrt(2) / math.sqrt(math.pi)) * SIGMA\n","\n","        return mu0 + W*sig0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nXLHtSLNd4K"},"outputs":[],"source":["def _pooling(ptype, num_features, kernel_size, stride, padding=0):\n","    if ptype == 'max':\n","        pool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n","    elif ptype == 'avg':\n","        pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n","    elif ptype == 'gauss_HWCN':\n","        pool = GaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding)\n","    elif ptype == 'gauss_CN':\n","        pool = GaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding, stochasticity='CN')\n","    elif ptype == 'gauss_half_HWCN':\n","        pool = halfGaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding)\n","    elif ptype == 'gauss_half_CN':\n","        pool = halfGaussianPooling2d(num_features=num_features, kernel_size=kernel_size, stride=stride, padding=padding, stochasticity='CN')\n","    else:\n","        raise ValueError(\"pooling type of {} is not supported\".format(ptype))\n","    return pool"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qlRohpENd4M"},"outputs":[],"source":["class Transition_layer(nn.Sequential):\n","  def __init__(self, nin, theta=0.5):    \n","      super(Transition_layer, self).__init__()\n","      \n","      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n","      self.add_module('{}'.format(ptype), _pooling(ptype=ptype, num_features=int(nin*theta), kernel_size=2, stride=2, padding=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIdoZbRXNd4M"},"outputs":[],"source":["class DenseBlock(nn.Sequential):\n","  def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n","      super(DenseBlock, self).__init__()\n","                        \n","      for i in range(num_bottleneck_layers):\n","          nin_bottleneck_layer = nin + growth_rate * i\n","          self.add_module('bottleneck_layer_%d' % i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdFkaVmjNd4N"},"outputs":[],"source":["class DenseNet(nn.Module):\n","    def __init__(self, growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10):\n","        super(DenseNet, self).__init__()\n","        \n","        assert (num_layers - 7) % 12 == 0\n","        \n","        # (num_layers-4)//6 \n","        num_bottleneck_layers = (num_layers - 7) // 12\n","        \n","        # 32 x 32 x 3 --> 32 x 32 x (growth_rate*2)\n","        self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=3, stride=1, padding=1, bias=True)\n","                \n","        # 32 x 32 x (growth_rate*2) --> 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","\n","        # 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)] --> 17 x 17 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n","        self.transition_layer_1 = Transition_layer(nin=nin_transition_layer_1, theta=theta)\n","        \n","        # 17 x 17 x nin_transition_layer_1*theta --> 17 x 17 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","\n","        # 17 x 17 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)] --> 9 x 9 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n","        self.transition_layer_2 = Transition_layer(nin=nin_transition_layer_2, theta=theta)\n","        \n","        # 9 x 9 x nin_transition_layer_2*theta --> 9 x 9 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","        \n","        # 9 x 9 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> 5 x 5 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_3 = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers)\n","        self.transition_layer_3 = Transition_layer(nin=nin_transition_layer_3, theta=theta)\n","        \n","        # 5 x 5 x nin_transition_layer_3*theta --> 5 x 5 x [nin_transition_layer_3*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_4 = DenseBlock(nin=int(nin_transition_layer_3*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","        \n","        # 5 x 5 x [nin_transition_layer_3*theta + (growth_rate * num_bottleneck_layers)] --> 3 x 3 x [nin_transition_layer_3*theta + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_4 = int(nin_transition_layer_3*theta) + (growth_rate * num_bottleneck_layers)\n","        self.transition_layer_4 = Transition_layer(nin=nin_transition_layer_4, theta=theta)\n","        \n","        # 3 x 3 x nin_transition_layer_4*theta --> 3 x 3 x [nin_transition_layer_4*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_5 = DenseBlock(nin=int(nin_transition_layer_4*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","        \n","        # 3 x 3 x [nin_transition_layer_4*theta + (growth_rate * num_bottleneck_layers)] --> 2 x 2 x [nin_transition_layer_4*theta + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_5 = int(nin_transition_layer_4*theta) + (growth_rate * num_bottleneck_layers)\n","        self.transition_layer_5 = Transition_layer(nin=nin_transition_layer_5, theta=theta)\n","        \n","        # 2 x 2 x nin_transition_layer_5*theta --> 2 x 2 x [nin_transition_layer_5*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_6 = DenseBlock(nin=int(nin_transition_layer_5*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","        \n","        nin_fc_layer = int(nin_transition_layer_5*theta) + (growth_rate * num_bottleneck_layers) \n","        \n","        # [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> num_classes\n","        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n","        \n","    def forward(self, x):\n","        dense_init_output = self.dense_init(x)\n","        \n","        dense_block_1_output = self.dense_block_1(dense_init_output)\n","        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n","        \n","        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n","        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n","        \n","        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n","        transition_layer_3_output = self.transition_layer_3(dense_block_3_output)\n","        \n","        dense_block_4_output = self.dense_block_4(transition_layer_3_output)\n","        transition_layer_4_output = self.transition_layer_4(dense_block_4_output)\n","        \n","        dense_block_5_output = self.dense_block_5(transition_layer_4_output)\n","        transition_layer_5_output = self.transition_layer_5(dense_block_5_output)\n","        \n","        dense_block_6_output = self.dense_block_6(transition_layer_5_output)\n","        \n","        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_6_output, (1, 1))                \n","        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n","\n","        output = self.fc_layer(global_avg_pool_output_flat)\n","        \n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuinGX_bNd4R"},"outputs":[],"source":["def DenseNetBC_103_12():\n","    return DenseNet(growth_rate=12, num_layers=103, theta=0.5, drop_rate=0.2, num_classes=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kgc8PXYNd4U"},"outputs":[],"source":["net = DenseNetBC_103_12()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXEpGZ8wNd4V"},"outputs":[],"source":["net.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"od8TlfGHNd4V"},"outputs":[],"source":["torchsummary.summary(net, (3, 32, 32))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzM9P0DHNd4W"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9)\n","lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[int(num_epoch * 0.5), int(num_epoch * 0.75)], gamma=0.1, last_epoch=-1)\n","\n","for epoch in range(num_epoch):  \n","    lr_scheduler.step()\n","    \n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        \n","        show_period = 100\n","        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n","            print('[%d, %5d/50000] loss: %.7f' %\n","                  (epoch + 1, (i + 1)*batch_size, running_loss / show_period))\n","            running_loss = 0.0\n","        \n","        \n","    # validation part\n","    correct = 0\n","    total = 0\n","    for i, data in enumerate(valid_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net(inputs)\n","        \n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        \n","    print('[%d epoch] Accuracy of the network on the validation images: %d %%' % \n","          (epoch + 1, 100 * correct / total)\n","         )\n","    torch.cuda.empty_cache()\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYIik6z-Nd4W"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"Densnet_total_6blocks.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}